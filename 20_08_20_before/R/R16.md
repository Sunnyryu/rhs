## R

#### R Study

```
분류 분석(Classification Analysis)
    다수의 변수를 갖는 데이터 셋을 대상으로 특정 변수의 값을 조건으로 지정하여 데이터를 분류하고 트리 형태의 모델을 생성하는 분석 방법
    의사결정트리(Decision Tree)
    랜덤 포레스트(Random Forest)
    인공신경망(Artificial Neural Network)
    고객을 분류하는 변수, 규칙, 특성을 찾아내고 이를 토대로 미래 잠재 고객의 행동이나 반응을 예측하거나 유도하는데 활용
    예) 과거의 환자들에 대한 종양 검사의 결과를 바탕으로 종양의 악성 또는 양성 여부를 분류하는 모델을 생성하여 새로운 환자에 대한 암을 진단하는데 이용 ( 분류 조건 : 종양의 크기, 모양, 색깔)
    예) 대출 은행에서 기존 고객들의 데이터를 활용하여 신용상태의 분류모델을 생성한 후 새로운 고객에 대하여 향후 신용상태를 예측하는 데 이용한다 ( 분류 모델 생성 규칙 : 기존 체납횟수, 대출금과 현재 고객의 수입 비율, 대출 사유 등)

 분류 분석(Classification Analysis) 특징
    Y 변수 존재 : 설명변수(x 변수)와 반응변수(y 변수)가 존재
    의사결정트리 : 분류 예측모델에 의해서 의사결정트리 형태로 데이터가 분류
    비모수 검정 : 선형성, 정규성, 등분산성 가정이 필요 없다
    추론 기능 : 유의수준 판단 기준이 없다 (추론 기능 없음)
    활용분야 : 이탈고객과 지속고객 분류, 신용상태의 좋고, 나쁨, 번호이동고객과 지속 고객 분류 등

 분류 분석(Classification Analysis) 절차
    학습 데이터 생성
    분류 알고리즘을 통해 예측 모델 생성
    검정 데이터를 통해 분류규칙의 모델 평가(모형 평가)
    새로운 데이터에 적용하여 결과 예측
  모형평가
     어떤 모형이 random하게 예측하는 모형보다 예측력이 우수한지, 고려된 여러 모형 중 어느 모형이 가장 좋은 예측력을 보유하고 있는지를 비교/분석하는 과정

 의사 결정 트리(Decision Tree)
     나무(Tree) 구조 형태로 분류결과를 도출
     입력변수 중 가장 영향력 있는 변수를 기준으로 이진분류하여 분류 결과를 나무 구조 형태로 시각화
     비교적 모델 생성이 쉽고, 단순, 명료하여 현업에서 많이 사용되는 지도학습 모델
     의사결정규칙을 도표화 하여 분류와 예측을 수행하는 분석방법
     party 패키지 ctree()
     rpart 패키지 rpart()



 party 패키지 ctree()  분류 결과 해석
     첫번째 번호는 반응변수(종속변수)에 대해서 설명변수(독립변수)가 영향을 미치는 중요 변수의 척도를 나타내는 수치로서 수치가 작을 수록 영향을 미치는 정도가 높고, 순서는 분기되는 순서를 의미
     두번째는 의사결정 트리의 노드명 (노드 번호 뒤에 * 기호가 오면 해당 노드가 마지막 노드를 의미)
     노드명 뒤에 해당 변수의 임계값이 조건식으로 옴
     세번째는 노드의 분기 기준(criterion)이 되는 수치
     네번째는 반응변수(종속변수)의 통계량(statistic)이 표시

```

```R
# 의사 결정 트리
str(airquality)     # 관측치 153개 , 변수 6개
#Ozone, Solar.R(태양광), Wind, Temp, Month, Day
#온도에 영향을 미치는 변수를 알아보기
formula <- Temp ~ Solar.R+Wind+Ozone

#분류모델 생성
air_ctree <- ctree(formula, data=airquality)
air_ctree

plot(air_ctree)
#온도에 가장 큰 영향을 미치는 첫번째 영향변수는 Ozone
# 두번째 영향변수는 Wind
# 오존량 37이하이면서 바람의 양이 15.5이상이면 평균온도는 63정도에 해당
#바람의 양이 15.5이하인 경우 평균 온도는 70이상으로 나타남
#태양광은 온도에 영향을 미치지 않는 것으로 분석됨



############iris 데이터 셋으로 분류 분석 수행#################
set.seed(1234)  #시드값을 적용하면 랜덤 값이 동일하게 생성
idx <- sample(1:nrow(iris), nrow(iris)*0.7)
train <- iris[idx, ]   #학습 데이터
test <- iris[-idx, ]   #검정 데이터

#종속변수는 Species, 독립변수는 ...
formula <- Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width

#분류모델 생성
iris_ctree <- ctree(formula, data=train)   
iris_ctree
plot(iris_ctree, type="simple")
plot(iris_ctree)
# 꽃종 분류에 가장 중요한 독립변수는 Petal.Length, Petal.Width


# 분류모델 평가 - 예측치 생성, 혼돈 매트릭스 생성
pred <- predict(iris_ctree, test)
table(pred, test$Species)

#분류정확도  95.56%
(16 + 15 +12) /nrow(test)    # 0.9555556
```
![Deepin스크린샷_select-area_20190923100505](https://i.imgur.com/EnSpSpl.png)

```R
#k겹 교차 검증(k-fold cross validation)
# 3겹, 2회 반복을 위한 샘플링

install.packages("cvTools")
library(cvTools)
cross <- cvFolds(nrow(iris), K=3, R=2)
str(cross)

cross #교차 검정 데이터 확인 !
length(cross$which)
dim(cross$subsets)
table(cross$which)
1  2  3
50 50 50

R=1:2
K=1:3
CNT= 0            #카운트 변수
ACC <- numeric()  #정확도 저장

for(r in R ) {
  cat('\n R=', r, '\n')
   for(k in K) {
      datas_idx <- cross$subsets[cross$which==k, r]
      test <- iris[datas_idx, ]   #테스트 데이터 생성
      cat('test:', nrow(test), '\n')

      formula <- Species ~ .
      train <- iris[-datas_idx, ]  #훈련 데이터 생성
      cat('train:', nrow(train), '\n')

      model <- ctree(formula, data=train)
      pred <- predict(model, test)
      t <- table(pred, test$Species)
      print(t)              #혼돈 메트릭스
      CNT <- CNT+1
      ACC[CNT] <- (t[1,1]+t[2,2]+t[3,3]) /sum(t)
      }
   }
CNT   #테스트 데이터 3셋 생성 모델, 예측 비교를 2번 반복, 즉 6회 수행
ACC  #6회 수행 정확도 확인
#6회 정확도의 평균
mean(ACC, na.rm=T)

```

```R

#ggplot(2::mpg) #데이터셋
#model(모델), dispi(엔진 크기), cyl(실린더수), drv(구동 방식)
# 종속 변수 : 고속도로 주행거리(hwy)
data(mpg)
t <-sample(1:nrow(mpg), 120)
train <- mpg[t, ]
test <- mpg[-t, ]
test$drv <- factor(test$drv)  #구동방식 범주형 변환
formula <- hwy ~ displ+cyl+drv
hwy_ctree <- ctree(formula, data=test)
plot(hwy_ctree)

분석 결과 : 엔진 크기가 작으면서 전륜구동(f)이나 후륜(r) 구동 방식인 경우 고속도로 주행거리가 가장 좋고,
엔진 크기가 크고, 사륜구동 방식이면 실린더 수가 많은 경우 고속도로 주행거리가 적은 것으로 분석된다.


```
![Deepin스크린샷_select-area_20190923105256](https://i.imgur.com/f4FiJK6.png)

```R


#10,000개 관측치를 샘플링해서
#자본이득에 영향을 미치는 변수를 분석하기 위해
#capital-gain, hours-per-week, education-num, race, age, income 변수로만 구성된 데이터프레임을 생성한후 분류모델 생성하고 예측하시오
names(AdultUCI)
set.seed(1234)
choice <- sample(1:nrow(AdultUCI), 10000)
choice
!is.na(choice)

adult.df <- AdultUCI[choice, ]
str(adult.df)

capital <- adult.df$'capital-gain'
hours <- adult.df$'hours-per-week'
education <- adult.df$'education-num'
race <- adult.df$race
age <- adult.df$age
income <- adult.df$income

adult_df <- data.frame(capital=capital, age=age , hours=hours,
    education=education, income=income)
str(adult_df)

formula <- capital ~ income+education+hours+age

adult_ctree <- ctree(formula, data=adult_df)

plot(adult_ctree)

#분석결과 : 자본이득(capital)에 가장 큰 영향을 미치는 변수는 income이고, 두번째는 education 변수이다.
#수입이 많고 교육수준이 높을수록 자본이득이 많은 것으로 분석된다.

#분류 모델의 조건에 맞는 subset 생성
adultResult <- subset(adult_df, adult_df$income=='large' &  adult_df$education > 14)
length(adultResult$education)
summary(adultResult$capital)
boxplot(adultResult$capital)

#income이 large이고 education이 14를 초과한 경우,
#자본이득 평균은 7,170


```

![Deepin스크린샷_select-area_20190923113833](https://i.imgur.com/cJacQcM.png)
![Deepin스크린샷_select-area_20190923113750](https://i.imgur.com/vMCy747.png)

```R
library(rpart)
data(iris)

iris.df <- rpart(Species ~., data=iris)
iris.df

plot(iris.df)
text(iris.df, use.n=T, cex=0.6)
post(iris.df, file="")

#줄기에 분기 조건
#끝 노드에는 반응변수의 결과값이 나타냄
# 꽃 종류 변수를 분류하는 가장 중요한 변수는 Petal.Length와 Petal.Width로 나타냄

```
![Deepin스크린샷_select-area_20190923114639](https://i.imgur.com/HT4Wgpz.png)

```R

#분류분석 연습문제 3
weather <- read.csv("./data/weather.csv", header = TRUE)
#RainTomorrow 컬럼을 종속 변수로
# 날씨 요인과 관련없는 Dateㄹ와 RainToday 컬럼을 제외한 나머지 변수를 x 변수로 지정하여 분류 모델 생성하고
weather <- read.csv("./data/weather.csv", header = TRUE)
weather
weather
w <-sample(1:nrow(weather), )  
train <- weather[w, ]
test <- weather[w, ]
formula <- RainTomorrow ~ MinTemp+MaxTemp+Rainfall+Sunshine+WindGustDir+WindGustSpeed+WindDir+WindSpeed+Humidity+Pressure+Cloud+Temp
w_ctree <- ctree(formula, data=test)
formula
plot(w_ctree)
w_ctree

str(weather)
names(weather)
weather.df <- rpart(RainTomorrow ~ ., data=weather[, c(-1, -14)], cp=0.01)
X11()
plot(weather.df)
text(weather.df, use.n=T, cex=0.7)

#분석 결과 : 분기조건이 True이면 왼쪽으로 분류되고, False

#rpart()함수의 cp속성값을 높이면 가지 수가 적어지고, 낮추면 가지 수가 많아진다. cp 기본값은 0.01

weather_pred <- predict(weather.df , weather)
weather_pred

#y의 범주로 코딩 변환 : Yes(0.5이상), No(0.5미만)
#rpart의 분류모델 예측치는 비 유무를 0~1사이의 확률값으로 예측하다
# 혼돈매트릭스를 이용하여 분류정확도를 구하기 위해 범주화 코딩 변경
weather_pred2 <- ifelse(weather_pred[,2] >= 0.5, 'Yes', 'No')
table(weather_pred2, weather$RainTomorrow)
(278+53)/nrow(weather) = 0.906
```
![Deepin스크린샷_select-area_20190923131301](https://i.imgur.com/RreUi2w.png)
![Deepin스크린샷_select-area_20190923131004](https://i.imgur.com/LK5ExEi.png)

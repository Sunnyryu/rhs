## R

#### R Study

```
랜덤 포레스트(Random Forest)
    의사결정트리에서 파생된 앙상블 학습기법을 적용한 모델
    앙상블 학습 기법 – 새로운 데이터에 대해서 여러 개의 트리(Forest)로 학습을 수행한 후 학습 결과들을 종합해서 예측하는 모델
    기존의 의사결정트리 방식에 비해서 많은 데이터를 이용하여 학습을 수행하기 때문에 비교적 예측력이 뛰어나고, 과적합(overfitting)문제를 해결할 수 있음
    과적합 문제 – 작은 데이터 셋은 높은 정확도가 나타나지만 큰 데이터셋에서는 정확도가 떨어지는 현상을 의미

랜덤 포레스트(Random Forest) 학습데이터 구성방법
    표본에서 일부분만 복원추출 방법으로 랜덤하게 샘플링하는 방식인 부트스트랩 표본(bootstrap sample) 방식으로 학습데이터로 사용될 트리(Forest)를 생성
    입력변수 중에서 일부 변수만 적용하여 트리의 자식노드(child node)를 분류

랜덤 포레스트(Random Forest)
    formula : y~x 형식으로 반응변수와 설명변수 식
    data : 모델 생성에 사용될 데이터 셋
    ntree :  복원추출하여 생성할 트리 수 지정
    mtry : 자식 노드를 분류할 변수 수 지정
    na.action :  결측치를 제거할 함수 지정
    importance : 분류모델 생성과정에서 중요 변수 정보 제공 여부
    randomForest(formula, data, ntree, mtry, na.action, importance)
```

```R
install.packages("randomForest")
library(randomForest)
data(iris)

model<-randomForest(Species~., data=iris)
model
setosa versicolor virginica class.error
setosa         50          0         0        0.00
versicolor      0         47         3        0.06
virginica       0          3        47        0.06

#Number of trees는 학습데이터(Forest)로 복원 추출 방식으로 500개 생성했다는 의미
#No, of variables tried at each split는 두 개의 변수를 이용하여 트리의 자식노드가 분류되었다는 의미 (ntree:500, mtry:2)
#error.rate는 모델의 분류정확도 오차 비율을 의미
#Confusion matrix (혼돈 메트릭스) ..
#분류 정확도는 (setosa+versicolor+virgina)/150 : 96%


model2<-randomForest(Species~., data=iris, ntree=300, mtry=4, na.action=na.omit)
model2

#중요변수 생성으로 랜덤 포레스트 모델 생성

model3<-randomForest(Species~., data=iris, importance=T, na.action=na.omit)
model3
#중요 변수 보기 - importance속성은 분류모델 생성하는 과정에서 입력변수 중 가장 중요한 변수가 어떤 변수인가를 알려주는 역할을 함.

importance(model3)

ntree <- c(400, 500, 600)

mtry <-c(2:4)
param <- data.frame(n=ntree, m=mtry)
param

for(i in param$n){
  cat('ntree=', i, '\n')
  for(j in param$m) {
    cat('mtry = ', j, '\n')
    model_iris <- randomForest(Species~. , data=iris, ntree=i, mtry=j, na.action=na.omit)
    print(model_iris)
  }
}
model_iris


```

```
인공신경망(Artificial Neural Network)
    인간의 두뇌 신경(뉴런)들이 상호작용하여 경험과 학습을 통해서 패턴을 발견하고,  발견된 패턴을 통해서 특정 사건을 일반화하거나 데이터를 분류하는데 이용되는 기계학습 방법
    인간의 개입 없이 컴퓨터가 스스로 인지하고 추론, 판단하여 사물을 구분하거나 특정 상황의 미래를 예측하는데 이용될 수 있는 기계학습 방법
    문자, 음성, 이미지 인식, 증권시장 예측, 날씨 예보 등 다양한 분야에서 활용
    예) 구글의 알파고(딥러닝)

생물학적 신경망 구조
    수상돌기(Dendrites) : 외부로부터 신경 자극을 받아들이는 역할
    시냅스(Synapse) : 신경과 신경의 연결 고리(뉴런 간의 교신)  신경과 신경 간의 신호 전달 기능으로 전달할 신호의 세기(Weight) 결정
    세포액(Soma) : 여러 신경으로부터 전달되는 신경 자극에 대한 판정과 다른 신경으로 신호 전달 여부 결정
    흑색돌기(Axon) : 전류와 비슷한 형태로 다른 신경으로 신호 전달 기능

    신경세포(뉴런)의 입력은 다수이고 출력은 하나이며, 여러 신경세포로부터 전달되어 온 신호들은 합산되어 출력, 합산된 값이 설정값 이상이면 출력 신호가 생기고 이하이면 출력 신호가 없음  

인공신경망과 비교
    인간의 생물학적 신경세포가 하나가 아닌 다수가 연결되어 의미 있는 작업을 하듯, 인공신경망의 경우도 개별 뉴런들을 서로 시냅스를 통해 서로 연결시켜서 복수개의 계층(layer)이 서로 연결되어 각 층간의 연결 강도는 가중치로 수정(update) 가능합니다. 이와 같이 다층 구조와 연결강도로 학습과 인지를 위한 분야에 활용

인공신경망과 비교2
    외부 신호를 받는 수상돌기는 컴퓨터에 입력 신호(X)에 해당되고, 시냅스는 입력 시호에 가중치를 적용하는 역할을 함
    세포핵은 입력 신호와 가중치를 이용하여 망의 총합(Σ)을 계산하고, 활성함수(f)를 이용하여 망의 총합을 출력 신호(y)에 보내는 역할을 함
    망의 총합은 입력 신호(X)와 가중치(W) 곱의 합 (Σ) 에 의해서 계산함
    활성함수(f)는 망의 총합을 받아서 축색돌기 출력 신호(y)를 전송하는 역할을 함

가중치 적용
    시냅스에서는 외부 신호 입력에 따라서 세기를 적용
    입력값 (x1, x2, x3,…n)은 수상돌기에 해당하는 외부 신경 자극에 해당하고, 가중치 (w1, w2, w3, ….주) 는 시냅스에 의해서 신호의 세기가 결정되는 부분에 해당
    입력 신호(X)와 일대일로 가중치(W)가 적용
    경계값(b:bias)은 활성 함수에 의해서 망의 총합을 다음 계층으로 넘길 때 영향을 주는 값
    입력 신호의 가중치는 중요 변수에 따라서 가중치가 달라지는데, 초기 가중치는 무작위(Random)로 생성되지만, 출력 값의 예측결과에 따라서 가중치는 수정(중요 변수의 가중치는 높게 설정)됨


퍼셉트론(Perceptron)
    여러 개의 계츠응로 다층화하여 만들어진 인공신경망
    퍼셉트론 모형에서 입력변수와 출력변수는 분석자가 지정함
    인공신경망은 은닉층에서 연산과정이 공개되지 않기 때문에 블랙박스 모형으로 분류되기도 함
    데이터 분류나 예측 결과는 제공하지만 어떠한 원인으로 결과가 도출되었는지에 대한 이유를 설명할 수 없는 모형
    여러 변수 간의 관계를 밝히는데 이 모형을 이용할 수 없음

퍼셉트론(Perceptron) 각 계층의 구성 요소
    입력값 : x1, x2, x3
    입력층 : 입력변수의 가중치(w)와 경계값(b)
    은닉층 : 입력층의 가중치(w)와 경계값(b)
    출력층 : 은직층의 가중치(w)와 경계값(b)
    출력값 : o1, o2, o3

인공신경망 기계학습 역전파 알고리즘)
    인공신경망에서 기계학습은 출력값(o1)과 실제 관측치(y1)을 비교하여 오차(E)를 계산하고, 이러한 오차를 줄이기 위해서 가중치(w)와 경계값(b)를 조절함
    예) 오차(E)가 0보다 큰 경우( E > 0)는  관측값에 비교하여 출력값이
    작다는 의미, 따라서 은닉층의 출력(hi)이 양수이면 가중치(wi)를 크게
    하고, 은닉층의 출력(hi)이 음수이면 가중치(wi)를 더 작게함
    또한, 오차(E)가 0보다 작은 경우( E < 0)는  관측값에 비교하여 출력값이
    작다는 의미, 따라서 은닉층의 출력(hi)이 양수이면 가중치(wi)를 더 작게
    하고, 은닉층의 출력(hi)이 음수이면 가중치(wi)를 더 크게함
    인공신경망(퍼셉트론)은 기본적으로 단방향 망(Feed Forward Network)으로 구성함 (입력층->은닉층->출력층)
    단방향 전파 방식을 개선하여 (은닉층<-출력층) 역방향으로 오차(E)를 전파하여 은닉층의 가중치와 경계값을 조정하여 분류정확도를 높이는 역전파(Backpropagation) 알고리즘을 도입하고 있음.
    역전파 알고리즘은 출력에서 생긴 오차를 신경망의 역방향(입력층)으로 전파하여 순차적으로 편미분을 수행하면서 가중치(w)와 경계값(b)등을 수정함
    입력값에 최적화된 가중치와 경계값이 적용되도록 구현된 인공신경망 관련된 알고리즘을 의미

    오차(E) = 관측값(y1) – 출력값(o1)
    nnet::nnet(formula, data, weights, size)

    formula : y ~ x 형식으로 반응변수(종속변수)와 설명변수(독립변수) 식
    data : 모델 생성에 사용될 데이터 셋
    weights : 각 case에 적용할 가중치(기본값 : 1)
    size : 은닉층(hidden layer)의 수 지정

인공신경망 기계학습 역전파 알고리즘)
    neuralnet 패키지는 역전파(Backpropagation) 알고리즘을 적용할 수 있고, 가중치 망을 시각화하는 기능도 제공
    출력변수(y)는 ‘yes’, ‘no’ 형태의 문자열이 아닌 1과 0의 수치형 이여야함

    nnet::neuralnet(formula, data, hidden=1, threshold=0.01, stepmax=1e+05, rep=1,
                    startweights = NULL, learnningrate=NULL, algorithm=“rprop+”)
    formula : y ~ x 형식으로 반응변수(종속변수)와 설명변수(독립변수) 식
    data : 모델 생성에 사용될 데이터 셋
    hidden : 은닉층(hidden layer)의 수 지정
    threshold : 경계값 지정
    stepmax : 인공신경망 학습을 위한 최대 스텝 지정
    rep : 인공신경망의 학습을 위한 반복 수 지정
    startweights : 랜덤으로 초기화된 가중치를 직접 지정
    learningrate : backpropagation 알고리즘에서 사용될 학습비율을 지정
    algorithm : backpropagation과 같은 알고리즘 적용을 위한 속성
    모델의 정확도를 평가하기 위해서 compute() 함수를 이용하여 모델의 예측치를 생성함

인공신경망 기계학습 역전파 알고리즘)
    분류모델의 성능을 향상시키기 위해서 은닉층의 노드(hidden)를 증가시키거나 역전파 알고리즘 등을 적용하여 분류모델의 성능을 높일 수 있음
    무조건 은닉층의 노드 수를 증가시킨다고 분류의 정확도가 높아지는 것은 아님
    Algorithm 속성에서 backprop 는 역전파를 통해서 가중치와 경계값을 조정하여 오차(E)를 줄이기 위해서 사용되는 속성임
```



```R
library(nnet)
df = data.frame(x2 = c(1:6), x1=c(6:1), y=factor(c('no', 'no','no', 'yes','yes','yes')))

model_net <- nnet(y~., df, size=1)
weights:  5
initial  value 4.191685
iter  10 value 0.058766
iter  20 value 0.000455
final  value 0.000055
converged
#결과는 5개의 가중치 생성 , 오차는 점차적으로 줄어드는 결과를 확인할 수 있음
model_net
a 2-1-1 network with 5 weights
inputs: x2 x1
output(s): y
options were - entropy fitting

#신경망 (a 2-1-1)은 (경계값-입력변수-은닉층-출력변수)

summary(model_net)  #가중치 요약 정보 확인
#입력층의 경계값(b) 1개와 입력변수(i1, i2)2개가 은닉층(h1)으로 연결되는 가중치
#은닉층의 경계값(b) 1개와 은닉층의 결과값이 출력층으로 연결되는 가중치

model_net$fitted.values  # 분류모델의 적합값


p <- predict(model_net, df, type="class")
#type="class"는 예측 결과를 출력변수 y의 범주('no','yes')로 분류
table(p, df$y)
p     no yes
  no   3   0
  yes  0   3

#iris 데이터 셋에 인공신경망 모델 생성
idx <- sample(1:nrow(iris), 0.7*nrow(iris))
training <- iris[idx, ]
testing <- iris[-idx, ]

model_net_iris <- nnet(Species~., training, size=1)
#은닉층 1개, 11개의 가증치, 출력값 3개
# weights:  11
initial  value 114.544510
iter  10 value 47.560690
iter  20 value 41.739864
iter  30 value 25.399222
iter  40 value 9.221907
iter  50 value 2.049612
iter  60 value 1.431509
iter  70 value 1.230628
iter  80 value 0.632811
iter  90 value 0.381637
iter 100 value 0.171284
final  value 0.171284
stopped after 100 iterations


model_net_iris_3 <- nnet(Species~., training, size=3)
#은닉층 3개, 27개의 가중치, 출력값 3개
# weights:  27
initial  value 117.218590
iter  10 value 34.123504
iter  20 value 0.627297
iter  30 value 0.000542
final  value 0.000097
converged
# 입력변수의 값들이 일정하지 않으면 과적합(overfitting)을 피하기 위해서 정규화 과정을 수행해야함

#가중치 확인
summary(model_net_iris)
summary(model_net_iris_3)

a 4-3-3 network with 27 weights
options were - softmax modelling
 b->h1 i1->h1 i2->h1 i3->h1 i4->h1
 35.72   9.67  14.95 -18.55 -28.12
 b->h2 i1->h2 i2->h2 i3->h2 i4->h2
  0.00   1.56   0.41   2.31   1.75
 b->h3 i1->h3 i2->h3 i3->h3 i4->h3
 -1.13  -1.46  -5.83   8.99   4.82
 b->o1 h1->o1 h2->o1 h3->o1
  3.80  12.27   4.66 -19.29
 b->o2 h1->o2 h2->o2 h3->o2
 -6.29  18.58  -6.74   9.04
 b->o3 h1->o3 h2->o3 h3->o3
  3.61 -30.16   2.55   9.61


# 분류모델의 정확도 평가
table(predict(model_net_iris, testing, type="class"), testing$Species)
#정확률은 0.9555556
(43)/45
table(predict(model_net_iris_3, testing, type="class"), testing$Species)
#정확률은 0.9555556
43/45


#nnet 패키지에서 제공되는 인공 신경망 모델은 1개의 은닉층으로 최적화되어 있기 때문에
은닉층의 노드 수를 3개로 늘려서 많은 연산이 수행된 반면, 분류 정확도는 크게 차이가 없다


#iris 데이터 셋에 인공신경망 모델 생성: neuralnet 패키지
library(neuralnet)
idx<-sample(1:nrow(iris), 0.7*nrow(iris))
training <- iris[idx, ]
testing <- iris[-idx, ]

#neuralnet()함수는 종속변수(출력변수y)가 수치형이어야 합니다.

training$Species2[training$Species=='setosa'] <- 1
training$Species2[training$Species=='versicolor'] <- 2
training$Species2[training$Species=='virginica'] <- 3
head(training)

testing$Species2[testing$Species=='setosa'] <- 1
testing$Species2[testing$Species=='versicolor'] <- 2
testing$Species2[testing$Species=='virginica'] <- 3
head(testing)
testing$Species <- NULL
training$Species <- NULL
# 정규화 함수를 이용하여 학습데이터와 검정데이터를 정규화
# 0과 1사이의 범위로 컬럼값을 정규화
normal <- function(x){
  return ((x-min(x)))/(max(x)-min(x))
}
min(x)
max(x)
x
training_nor <- as.data.frame(lapply(training, normal))
summary(training_nor)

testing_nor <- as.data.frame(lapply(testing, normal))
summary(testing_nor)

#인공신경망 분류 모델 생성
model_net <- neuralnet(Species2 ~ Sepal.Length+ Sepal.Width+Petal.Length+Petal.Width, data=training_nor, hidden=1)
model_net
plot(model_net)

#분류모델 정확도(성능) 평가
model_result <- compute(model_net, testing_nor[c(1:4)])
model_result$net.result

#상관관계분석 : 상관계수로 두 변수 간의 선형관계의 강도 측정
#예측된 꼭 종류와 실제 관측치 사이의 상관관계 측정
cor(model_result$net.result, testing_nor$Species2)

#은닉층 2개
model_net2 <- neuralnet(Species2 ~ Sepal.Length+ Sepal.Width+Petal.Length+Petal.Width,
                        data=training_nor, hidden=2)
model_net2
plot(model_net2)


#분류모델 정확도(성능) 평가
model_result2 <- compute(model_net, testing_nor[c(1:4)])
model_result2$net.result
cor(model_result2$net.result, testing_nor$Species2)
0.983542
```
![Deepin스크린샷_select-area_20190923153638](https://i.imgur.com/B7MxHp0.png)
![Deepin스크린샷_select-area_20190923153646](https://i.imgur.com/wj2R480.png)
![Deepin스크린샷_select-area_20190923153708](https://i.imgur.com/Hy5FRaT.png)

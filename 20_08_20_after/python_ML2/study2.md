## study

```

초기 머신러닝은 뉴런개념으로 접근했다.

그 후에 퍼셉트론 학습 개념이 나왔고 가중치를 학습하는 알고리즘이 나왔다. (지도학습과 분류 개념으로 말하면 이 알고리즘으로 샘플이 한 클래스에 속하는지 아닌지 알수있음!)

인공 뉴런은 두개의 클래스가 있는 이진 분류 작업으로 볼수 있음!

특정 샘플의 최종 입력이 사전에 정의 된 임계 값보다 크면 1이라고 예측하고 아니면 -1이라고 할 수 있다!

퍼셉트론 알고리즘에서 결정함수는 단위 계단 함수를 변형한 것!(단위 계단함 수는 z>=0일때 1, 나머지는 0이다.)

z = w^tX

머신러닝 분야에서는 음수 임계 값또는 가중치 Wo = 기호를 절편이라고 함!

퍼셉트론 모델 이면에 있는 전반적인 아이디어는 뇌의 뉴런 하나가 작동하는 방식을 흉내내려는 환원주의 접근 방식을 사용한 것입니다. 즉 출력을 내거나 내지 않는 두 가지 경우만 있음! (가중치를 0 또는 랜덤한 작은 값으로 초기화함! / 각 훈련 샘플 x^(i)에서 다음 작업을 함 / 출력 값 y를 계산 -> 가중치를 업데이트함!)

출력 값은 앞서 정의한 단위 계딴 함수로 예측한 클래스 레이블 / 가중치 벡터 w에 있는 개별 가중치 W_f가 동시에 업데이트됨

가중치를 업데이트 하는 값은 학습률(0~1 사이의 정수) 
y^(i)는 i번쨰 훈련 샘플의 진짜 클래스 레이블 / (^y)는 에측 클래스 레이블임. 가중치 벡터와 모든 가중치를 동시에 업데이트한다는 점이 중요, 모든 가중치를 업데이트 하기 전에 예측 클래스 레이블을 다시 계산하지 않음 / 2차원 데이터셋에서는 다음과 같이 업데이트! / 진짜 클래스 레이블에서 아웃풋을 뺴주고 각 훈련샘플과 학습률을 곱해줌! 

잘못 예측 시에는 가중치를 양성 또는 음성 타깃 클래스 방향으로 이동시킴 

가중치를 잘못하면 값이 많이 달라지기에 중요시 해야함!
(가중치 업데이트는 훈련샘플의 값에 비례함!)
잘못 분류한 값이 커질 수록 결정 경계를 더 크게 움직임!

퍼셉트론은 두 클래스가 선형적으로 구분되고 학습률이 충분히 작을 때만 수렴이 보장됩니다. 두 클래스를 선형 결정 경계로 나눌 수 없다면 훈련 데이터 셋을 반복할 최대 횟수를 지정하고 분류 허용 오차를 지정할 수 있음! (최대회수는 에포크!) / 그렇지 않으면 퍼셉트론은 가중치 업데이트를 멈추지 않음! 

1, x1,x2...xm -> w0,x1...wm -> 최종입력 함수 -> 입게함수 -> 오차가 발생하면 가중치 업데이트 실행 -> 오차가 없다면 출력!
```

```python

import numpy as np

class Percentron(object):
    """
    퍼셉트론 분류기

    매개변수
    ---------------
    eta : float 
    학습률 (0.0과 1.0사이)
    n_iter : int
    훈련 데이터셋 반복 횟수
    random_state : int
    가중치 무작위 초기화를 위한 난수 생성기 시드

    속성
    ----------------
    w_ : 1d-array(학습된 가중치)
    errors_ : list (에포크- (최대 회수)마다 누적된 분류 오류)
    """
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta
        self.n_iter =n_iter
        self.random_state= random_state
    def fit(self, X, y):
        """훈련된 데이터 학습

        매개변수
        ------------
        X : array -like, shape = [n_samples, n_features]
        n_samples개의 샘플과 n_features개의 특성으로 이루어진 훈련데이터
        y : array-like, shape= [n_samples]
        타깃 값 

        반환 값
        -----------
        self : object
        """
        regn = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size= 1 + X.shape[1])

        self.errors_ = []

        for _ in range(self.n_iter):
            errors = 0
            for xi, target in zip(X,y):
                update = self.eta * (target - self.predict(xi))
                self.w_[1:] += update * xi
                self.w_[0] += update
                errors += int(update != 0.0)
            self.errors_.append(errors)
        return self
    
    def net_input(self, X):
        """최종계산"""
        return np.dot(X, self.w_[1:]) + self.w_[0]
    
    def predict(self,X):
        """단위 계단 함수를 사용하여 클래스 레이블을 반환합니다."""
        return np.where(self.net_input(X) >= 0.0, 1, -1)
```

```

먼저 위의 코드에서 학습률 eta와 에포크 횟수(훈련데이터를 반복하는 횟수) n_iter로 새로운 퍼셉트론 객체를 초기화 시킴 / fit 함수에서 self.w_ 가중치를 벡터 R^m+1로 초기화 함 / m은 데이터 셋에 있는 차원(특성) 개수! / 벡터의 첫 번째 원소인 절편을 위해 1을 더함 (이 벡터의 첫 번째 원소 self.w_[0]는 앞서 언급한 절편!!)

이 벡터는 regen.nomal을 사용하여 표준 편차가 0.01인 정규 분포에서 뽑은 랜덤한 작은 수를 담고 있음 / rgen은 넘파이 난수 생성기로 사용자가 지정한 랜덤 시드(이전과 동일한 것 재현 가능!)

가중치를 0으로 초기화하지 않은 이유는 가중치가 0이 아니여야 학습률이 분류 결과에 영항을 줌! / 0이라면 eta가 벡터의 크기에만 영향을 줌! 

fit 메서드는 가중치를 초기화한 후 훈련 세트에 있는 모든 개개의 샘플을 반복 순회 -> 가중치를 업데이트 / 클래스 레이블은 predict 함수에서 예측 / fit 함수에서 가중치를 업데이트 하기 위해 predict 메서드를 호출하여 클래스 레이블에 대한 예측을 얻음! / predict 메서드는 모델이 학습되고 난 후 새로운 데이터의 클래스 레이블을 예측하는 데도 사용할 수 있음! (최대 회수 마다 잘못 분류된 횟수에 기록!) / net_input 함수에서 사용한 np.dot 함수는 벡터 점곱 w^T * x를 계산함 
```


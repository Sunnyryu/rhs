{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f9706b",
   "metadata": {},
   "source": [
    "## 머신러닝 평가\n",
    "\n",
    "검증용 데이터는 모델의 성능을 평가하기 위한 용도가 아니라, 모델의 성능을 조정하기 위한 용도입니다. 더 정확히는 과적합이 되고 있는지 판단하거나 하이퍼파라미터의 조정을 위한 용도입니다. 하이퍼파라미터(초매개변수)란 값에 따라서 모델의 성능에 영향을 주는 매개변수들을 말합니다. 반면, 가중치와 편향과 같은 학습을 통해 바뀌어져가는 변수를 이 책에서는 매개변수라고 부름 ( 하이퍼파라미터와 매개변수의 가장 큰 차이는 하이퍼파라미터는 보통 사용자가 직접 정해줄 수 있는 변수)\n",
    "\n",
    "훈련용 데이터로 훈련을 모두 시킨 모델은 검증용 데이터를 사용하여 정확도를 검증하며 하이퍼파라미터를 튜닝(tuning)\n",
    "\n",
    "하이퍼파라미터 튜닝이 끝났다면, 이제 검증용 데이터로 모델을 평가하는 것은 적합하지 않습니다. 이제 모델은 검증용 데이터에 대해서도 일정 부분 최적화가 되어있기 때문입니다. 모델에 대한 평가는 모델이 아직까지 보지 못한 데이터로 하는 것이 가장 바람직\n",
    "\n",
    "테스트 데이터를 가지고 모델의 진짜 성능을 평가합니다. 비유하자면 훈련 데이터는 문제지, 검증 데이터는 모의고사, 테스트 데이터는 실력을 최종적으로 평가하는 수능 시험\n",
    "검증 데이터와 테스트 데이터를 나눌 만큼 데이터가 충분하지 않다면 k-폴드 교차 검증이라는 또 다른 방법을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2463be1c",
   "metadata": {},
   "source": [
    "### 분류 / 회귀\n",
    "\n",
    "선형 회귀를 통해 회귀 문제에 대해서 학습하고, 로지스틱 회귀를 통해 (이름은 회귀이지만) 분류 문제를 학습\n",
    "분류는 또한 이진 분류(Binary Classification)과 다중 클래스 분류(Multi-Class Classification)로 나뉩니다. 엄밀히는 다중 레이블 분류(Multi-lable Classification)라는 또 다른 문제가 존재\n",
    "\n",
    "이진 분류 문제(이진 분류는 주어진 입력에 대해서 둘 중 하나의 답을 정하는 문제입니다. 시험 성적에 대해서 합격, 불합격인지 판단하고 메일로부터 정상 메일, 스팸 메일인지를 판단하는 문제)\n",
    "\n",
    "다중 클래스 분류(다중 클래스 분류는 주어진 입력에 대해서 두 개 이상의 정해진 선택지 중에서 답을 정하는 문제입니다. 예를 들어 서점 아르바이트를 하는데 과학, 영어, IT, 학습지, 만화라는 레이블이 각각 붙여져 있는 5개의 책장이 있다고 합시다. 새 책이 입고되면, 이 책은 다섯 개의 책장 중에서 분야에 맞는 적절한 책장에 책을 넣어야 함->다섯 개의 선택지를 주로 카테고리 또는 범주 또는 클래스라고 하며, 주어진 입력으로부터 정해진 클래스 중 하나로 판단하는 것을 다중 클래스 분류 문제)\n",
    "\n",
    "회귀 문제(회귀 문제는 분류 문제처럼 0 또는 1이나 과학 책장, IT 책장 등과 같이 분리된(비연속적인) 답이 결과가 아니라 연속된 값을 결과로 가집니다. 예를 들어 시험 성적을 예측하는데 5시간 공부하였을 때 80점, 5시간 1분 공부하였을 때는 80.5점, 7시간 공부하였을 때는 90점 등이 나오는 것과 같은 문제)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d037c6fb",
   "metadata": {},
   "source": [
    "### 지도학습 && 비지도 학습\n",
    "머신러닝은 지도 학습, 비지도 학습, 강화 학습으로 나뉨\n",
    "\n",
    "지도학습(지도 학습이란 레이블(Label)이라는 정답과 함께 학습하는 것)\n",
    "비지도학습(비지도 학습은 레이블 없이 학습하는 것을 말함)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd6988",
   "metadata": {},
   "source": [
    "### 샘플 과 특성\n",
    "\n",
    "많은 머신 러닝 문제가 1개 이상의 독립 변수 x를 가지고 종속 변수 y를 예측하는 문제입니다. 많은 머신 러닝 모델들, 특히 인공 신경망 모델은 독립 변수, 종속 변수, 가중치, 편향 등을 행렬 연산을 통해 연산하는 경우\n",
    "\n",
    "머신 러닝에서는 하나의 데이터, 하나의 행을 샘플(Sample)이라고 부릅니다. (데이터베이스에서는 레코드라고 부르는 단위입니다.) 종속 변수 y를 예측하기 위한 각각의 독립 변수 x를 특성(Feature)이라고 부름\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27efa63",
   "metadata": {},
   "source": [
    "### 혼동행렬 \n",
    "\n",
    "정확도는 맞춘 결과와 틀린 결과에 대한 세부적인 내용을 알려주지는 않습니다. 이를 위해서 사용하는 것이 혼동 행렬(Confusion Matrix)입\n",
    "\n",
    "이를 각각 TP(True Positive), TN(True Negative), FP(False Postivie), FN(False Negative)라고 하는데 True는 정답을 맞춘 경우고 False는 정답을 맞추지 못한 경우입니다. 그리고 Positive와 Negative는 각각 제시했던 정답입니다. 즉, TP는 양성(Postive)이라고 대답하였고 실제로 양성이라서 정답을 맞춘 경우입니다. TN은 음성(Negative)이라고 대답하였는데 실제로 음성이라서 정답을 맞춘 경우입니다.\n",
    "\n",
    "그럼 FP는 양성이라고 대답하였는데, 음성이라서 정답을 틀린 경우이며 FN은 음성이라고 대답하였는데 양성이라서 정답을 틀린 경우가 됩니다. 그리고 이 개념을 사용하면 또 새로운 개념인 정밀도(Precision)과 재현율(Recall)\n",
    "\n",
    "#### 정밀도(Precision)\n",
    "\n",
    "정밀도=TP / (TP+FP)\n",
    "\n",
    "#### 재현율(Recall)\n",
    "\n",
    "재현율=TP / (TP+FN)\n",
    "\n",
    "### 과적합(Overfitting)과 과소 적합(Underfitting)\n",
    "\n",
    "과적합(Overfitting)이란 훈련 데이터를 과하게 학습한 경우를 말함\n",
    "테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한 상태를 반대로 과소적합이라고함\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0381eb6",
   "metadata": {},
   "source": [
    "## 선형회귀\n",
    "\n",
    "시험 공부하는 시간을 늘리면 늘릴 수록 성적이 잘 나옵니다. 하루에 걷는 횟수를 늘릴 수록, 몸무게는 줄어듭니다. 집의 평수가 클수록, 집의 매매 가격은 비싼 경향이 있습니다. 이는 수학적으로 생각해보면 어떤 요인의 수치에 따라서 특정 요인의 수치가 영향을 받고있다고 말함\n",
    "\n",
    "변수 x의 값은 독립적으로 변할 수 있는 것에 반해, y값은 계속해서 x의 값에 의해서, 종속적으로 결정되므로 x를 독립 변수, y를 종속 변수라고도 합니다. 선형 회귀는 한 개 이상의 독립 변수 x와 y의 선형 관계를 모델링합니다. 만약, 독립 변수 x가 1개라면 단순 선형 회귀\n",
    "\n",
    "### 단순 선형 회귀 분석\n",
    "y = Wx + b (simple Linear Regression Analysis)\n",
    "\n",
    "### 다중 선형 회귀 분석(multiple linear regression analysis)\n",
    "y=W1x1+W2x2+...Wnxn+b\n",
    "\n",
    "\n",
    "### 가설 세우기\n",
    "\n",
    "x와 y의 관계를 유추하기 위해서 수학적으로 식을 세워보게 되는데 머신 러닝에서는 이러한 식을 가설(Hypothesis)\n",
    "선형 회귀는 주어진 데이터로부터 y와 x의 관계를 가장 잘 나타내는 직선을 그리는 일\n",
    "\n",
    "### 비용함수 (평균 제곱오차 - Mean Squared Error)\n",
    "\n",
    "앞서 주어진 데이터에서 x와 y의 관계를 W와 b를 이용하여 식을 세우는 일을 가설이라고 언급했습니다. 그리고 이제 해야할 일은 문제에 대한 규칙을 가장 잘 표현하는 W와 b를 찾는 일입니다. 머신 러닝은 W와 b를 찾기 위해서 실제값과 가설로부터 얻은 예측값의 오차를 계산하는 식을 세우고, 이 식의 값을 최소화하는 최적의 W와 b를 찾아냄\n",
    "\n",
    "실제값과 예측값에 대한 오차에 대한 식을 목적 함수(Objective function) 또는 비용 함수(Cost function) 또는 손실 함수(Loss function)라고 함\n",
    "회귀 문제의 경우에는 주로 평균 제곱 오차(Mean Squared Error, MSE)가 사용\n",
    "\n",
    "모든 점들과의 오차가 클 수록 평균 제곱 오차는 커지며, 오차가 작아질 수록 평균 제곱 오차는 작아집니다. 그러므로 이 평균 최곱 오차. 즉, Cost(W,b)를 최소가 되게 만드는 W와 b를 구하면 결과적으로 y와 x의 관계를 가장 잘 나타내는 직선을 그릴 수 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6522a9f",
   "metadata": {},
   "source": [
    "### 옵티마이저 : 경사하강법\n",
    "\n",
    "선형 회귀를 포함한 수많은 머신 러닝, 딥 러닝의 학습은 결국 비용 함수를 최소화하는 매개 변수인 W와 b을 찾기 위한 작업을 수행합니다. 이때 사용되는 알고리즘을 옵티마이저(Optimizer) 또는 최적화 알고리즘\n",
    "\n",
    "기계는 임의의 랜덤값 W값을 정한 뒤에, 맨 아래의 볼록한 부분을 향해 점차 W의 값을 수정해나갑니다. 위의 그림은 W값이 점차 수정되는 과정을 보여줍니다. 그리고 이를 가능하게 하는 것이 경사 하강법(Gradient Descent)!\n",
    "\n",
    "경사 하강법은 미분을 배우게 되면 가장 처음 배우게 되는 개념인 한 점에서의 순간 변화율 또는 다른 표현으로는 접선에서의 기울기의 개념을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd51ca69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=8.0>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "w = tf.Variable(2.)\n",
    "\n",
    "def f(w):\n",
    "  y = w**2\n",
    "  z = 2*y + 5\n",
    "  return z\n",
    "with tf.GradientTape() as tape:\n",
    "  z = f(w)\n",
    "\n",
    "gradients = tape.gradient(z, [w])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a1e48fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15. 21. 23. 25.]\n",
      "epoch :   0 | W의 값 : 8.2133 | b의 값 : 1.664 | cost : 1402.555542\n",
      "epoch :  10 | W의 값 : 10.4971 | b의 값 : 1.977 | cost : 1.351182\n",
      "epoch :  20 | W의 값 : 10.5047 | b의 값 :  1.93 | cost : 1.328165\n",
      "epoch :  30 | W의 값 : 10.5119 | b의 값 : 1.884 | cost : 1.306967\n",
      "epoch :  40 | W의 값 : 10.5188 | b의 값 : 1.841 | cost : 1.287436\n",
      "epoch :  50 | W의 값 : 10.5254 | b의 값 : 1.799 | cost : 1.269459\n",
      "epoch :  60 | W의 값 : 10.5318 | b의 값 : 1.759 | cost : 1.252898\n",
      "epoch :  70 | W의 값 : 10.5379 | b의 값 : 1.721 | cost : 1.237644\n",
      "epoch :  80 | W의 값 : 10.5438 | b의 값 : 1.684 | cost : 1.223598\n",
      "epoch :  90 | W의 값 : 10.5494 | b의 값 : 1.648 | cost : 1.210658\n",
      "epoch : 100 | W의 값 : 10.5548 | b의 값 : 1.614 | cost : 1.198740\n",
      "epoch : 110 | W의 값 : 10.5600 | b의 값 : 1.582 | cost : 1.187767\n",
      "epoch : 120 | W의 값 : 10.5650 | b의 값 :  1.55 | cost : 1.177665\n",
      "epoch : 130 | W의 값 : 10.5697 | b의 값 :  1.52 | cost : 1.168354\n",
      "epoch : 140 | W의 값 : 10.5743 | b의 값 : 1.492 | cost : 1.159782\n",
      "epoch : 150 | W의 값 : 10.5787 | b의 값 : 1.464 | cost : 1.151890\n",
      "epoch : 160 | W의 값 : 10.5829 | b의 값 : 1.437 | cost : 1.144619\n",
      "epoch : 170 | W의 값 : 10.5870 | b의 값 : 1.412 | cost : 1.137924\n",
      "epoch : 180 | W의 값 : 10.5909 | b의 값 : 1.387 | cost : 1.131752\n",
      "epoch : 190 | W의 값 : 10.5946 | b의 값 : 1.364 | cost : 1.126073\n",
      "epoch : 200 | W의 값 : 10.5982 | b의 값 : 1.341 | cost : 1.120843\n",
      "epoch : 210 | W의 값 : 10.6016 | b의 값 :  1.32 | cost : 1.116026\n",
      "epoch : 220 | W의 값 : 10.6049 | b의 값 : 1.299 | cost : 1.111589\n",
      "epoch : 230 | W의 값 : 10.6081 | b의 값 : 1.279 | cost : 1.107504\n",
      "epoch : 240 | W의 값 : 10.6111 | b의 값 :  1.26 | cost : 1.103736\n",
      "epoch : 250 | W의 값 : 10.6140 | b의 값 : 1.242 | cost : 1.100273\n",
      "epoch : 260 | W의 값 : 10.6168 | b의 값 : 1.224 | cost : 1.097082\n",
      "epoch : 270 | W의 값 : 10.6195 | b의 값 : 1.207 | cost : 1.094143\n",
      "epoch : 280 | W의 값 : 10.6221 | b의 값 : 1.191 | cost : 1.091434\n",
      "epoch : 290 | W의 값 : 10.6245 | b의 값 : 1.176 | cost : 1.088940\n",
      "epoch : 300 | W의 값 : 10.6269 | b의 값 : 1.161 | cost : 1.086645\n",
      "[38.35479  54.295143 59.608593 64.92204 ]\n"
     ]
    }
   ],
   "source": [
    "# 학습될 가중치 변수를 선언\n",
    "W = tf.Variable(4.0)\n",
    "b = tf.Variable(1.0)\n",
    "@tf.function\n",
    "def hypothesis(x):\n",
    "  return W*x + b\n",
    "x_test = [3.5, 5, 5.5, 6]\n",
    "print(hypothesis(x_test).numpy())\n",
    "@tf.function\n",
    "def mse_loss(y_pred, y):\n",
    "  # 두 개의 차이값을 제곱을 해서 평균을 취한다.\n",
    "  return tf.reduce_mean(tf.square(y_pred - y))\n",
    "X=[1,2,3,4,5,6,7,8,9] # 공부하는 시간\n",
    "y=[11,22,33,44,53,66,77,87,95] # 각 공부하는 시간에 맵핑되는 성적\n",
    "optimizer = tf.optimizers.SGD(0.01)\n",
    "for i in range(301):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # 현재 파라미터에 기반한 입력 x에 대한 예측값을 y_pred\n",
    "    y_pred = hypothesis(X)\n",
    "\n",
    "    # 평균 제곱 오차를 계산\n",
    "    cost = mse_loss(y_pred, y)\n",
    "\n",
    "  # 손실 함수에 대한 파라미터의 미분값 계산\n",
    "  gradients = tape.gradient(cost, [W, b])\n",
    "\n",
    "  # 파라미터 업데이트\n",
    "  optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "\n",
    "  if i % 10 == 0:\n",
    "    print(\"epoch : {:3} | W의 값 : {:5.4f} | b의 값 : {:5.4} | cost : {:5.6f}\".format(i, W.numpy(), b.numpy(), cost))\n",
    "x_test = [3.5, 5, 5.5, 6]\n",
    "print(hypothesis(x_test).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a31c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 언어 모델 \n",
    "\n",
    "언어 모델(Language Model, LM)은 언어라는 현상을 모델링하고자 단어 시퀀스(또는 문장)에 확률을 할당(assign)하는 모델\n",
    "언어 모델을 만드는 방법은 크게는 통계를 이용한 방법과 인공 신경망을 이용한 방법으로 구분할 수 있음\n",
    "\n",
    "자연어 처리의 신기술인 GPT나 BERT 또한 인공 신경망 언어 모델의 개념을 사용\n",
    "(언어 모델은 단어 시퀀스에 확률을 할당(assign)하는 일을 하는 모델)\n",
    " 언어 모델이 이전 단어들이 주어졌을 때 다음 단어를 예측하도록 하는 것\n",
    "언어 모델에 -ing를 붙인 언어 모델링(Language Modeling)은 주어진 단어들로부터 아직 모르는 단어를 예측하는 작업을 말함\n",
    "(언어 모델은 위와 같이 확률을 통해 보다 적절한 문장을 판단 => 기계번역, 오타교정, 음성인식등)\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "비행기를 타려고 공항에 갔는데 지각을 하는 바람에 비행기를 [?]라는 문장이 있습니다. '비행기를' 다음에 어떤 단어가 오게 될지 사람은 쉽게 '놓쳤다'라고 예상할 수 있음\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 통계적 언어 모델\n",
    "통계적 언어 모델(Statistical Language Model)은 줄여서 SLM\n",
    "\n",
    "p(B|A)=P(A,B)/P(A)\n",
    " \n",
    "P(A,B)=P(A)P(B|A)\n",
    "P(A,B,C,D)=P(A)P(B|A)P(C|A,B)P(D|A,B,C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 'An adorable little boy is spreading smiles'의 확률 P(An adorable little boy is spreading smiles)를 식으로 표현\n",
    "\n",
    "P(An adorable little boy is spreading smiles)=\n",
    "P(An)×P(adorable|An)×P(little|An adorable)×P(boy|An adorable little)×P(is|An adorable little boy) ×P(spreading|An adorable little boy is)×P(smiles|An adorable little boy is spreading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장의 확률을 구하기 위해서 다음 단어에 대한 예측 확률을 모두 곱한다는 것은 알았습니다. 그렇다면 SLM은 이전 단어로부터 다음 단어에 대한 확률은 어떻게 구할까요? 정답은 카운트에 기반하여 확률을 계산합니다.\n",
    "\n",
    "An adorable little boy가 나왔을 때, is가 나올 확률인 P(is|An adorable little boy)를 구해봅시다.\n",
    "\n",
    "P(is|An adorable little boy)=count(An adorable little boy is)count(An adorable little boy )\n",
    "그 확률은 위와 같습니다. 예를 들어 기계가 학습한 코퍼스 데이터에서 An adorable little boy가 100번 등장했는데 그 다음에 is가 등장한 경우는 30번이라고 합시다. 이 경우 P(is|An adorable little boy)는 30%입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시 말해 기계가 훈련하는 데이터는 정말 방대한 양이 필요합니다.\n",
    "분한 데이터를 관측하지 못하여 언어를 정확히 모델링하지 못하는 문제를 희소 문제(sparsity problem)라고 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gran 언어 모델\n",
    "n-gram 언어 모델은 여전히 카운트에 기반한 통계적 접근을 사용하고 있으므로 SLM의 일종\n",
    "일부 단어를 몇 개 보느냐를 결정하는데 이것이 n-gram에서의 n이 가지는 의미\n",
    "\n",
    "unigrams : an, adorable, little, boy, is, spreading, smiles\n",
    "bigrams : an adorable, adorable little, little boy, boy is, is spreading, spreading smiles\n",
    "trigrams : an adorable little, adorable little boy, little boy is, boy is spreading, is spreading smiles\n",
    "4-grams : an adorable little boy, adorable little boy is, little boy is spreading, boy is spreading smiles\n",
    "\n",
    "n-gram을 통한 언어 모델에서는 다음에 나올 단어의 예측은 오직 n-1개의 단어에만 의존합니다. 예를 들어 'An adorable little boy is spreading' 다음에 나올 단어를 예측하고 싶다고 할 때, n=4라고 한 4-gram을 이용한 언어 모델을 사용한다고 합시다. 이 경우, spreading 다음에 올 단어를 예측하는 것은 n-1에 해당되는 앞의 3개의 단어만을 고려\n",
    "\n",
    "만약 갖고있는 코퍼스에서 boy is spreading가 1,000번 등장했다고 합시다. 그리고 boy is spreading insults가 500번 등장했으며, boy is spreading smiles가 200번 등장했다고 합시다. 그렇게 되면 boy is spreading 다음에 insults가 등장할 확률은 50%이며, smiles가 등장할 확률은 20%입니다. 확률적 선택에 따라 우리는 insults가 더 맞다고 판단하게 됩니다.\n",
    "\n",
    "P(insults|boy is spreading)=0.500\n",
    "P(smiles|boy is spreading)=0.200\n",
    "\n",
    "n-gram은 뒤의 단어 몇 개만 보다 보니 의도하고 싶은 대로 문장을 끝맺음하지 못하는 경우가 생긴다는 점\n",
    "문장에 존재하는 앞에 나온 단어를 모두 보는 것보다 일부 단어만을 보는 것으로 현실적으로 코퍼스에서 카운트 할 수 있는 확률을 높일 수는 있었지만, n-gram 언어 모델도 여전히 n-gram에 대한 희소 문제가 존재\n",
    "\n",
    "한국어는 어순이 중요하지 않고 교착어이며 띄어쓰기가 제대로 지켜지지 않는 경우가 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 펄플렉서터\n",
    "모델 내에서 자신의 성능을 수치화하여 결과를 내놓는 내부 평가(Intrinsic evaluation)에 해당되는 펄플렉서티\n",
    "펄플렉서티(perplexity)는 언어 모델을 평가하기 위한 내부 평가 지표(PPL)\n",
    "\n",
    "PPL은 선택할 수 있는 가능한 경우의 수를 의미하는 분기계수\n",
    "PPL은 이 언어 모델이 특정 시점에서 평균적으로 몇 개의 선택지를 가지고 고민하고 있는지를 의미\n",
    "PPL이 더 낮은 언어 모델의 성능이 더 좋다고 볼 수 있음.\n",
    "평가 방법에 있어서 주의할 점은 PPL의 값이 낮다는 것은 테스트 데이터 상에서 높은 정확도를 보인다는 것이지, 사람이 직접 느끼기에 좋은 언어 모델이라는 것을 반드시 의미하진 않는다는 점\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 조건부 확률 \n",
    "\n",
    "A = 학생이 남학생인 사건\n",
    "B = 학생이 여학생인 사건\n",
    "C = 학생이 중학생인 사건\n",
    "D = 학생이 고등학생인 사건\n",
    "\n",
    "남 중학생(100), 고등학생 (80)\n",
    "여 중학생(60), 고등학생(120)\n",
    "학생일떄 남학생일 확률 180/360 =0.5\n",
    "학생일때 고등학생에 남자일 확률 80/360 = 0.22\n",
    "\n",
    "고등학생 중 한명을 봅았을떄 남학생일 확률 P(A|D) = 80 /200 = P(A∩D)/P(D)  = (80/360)/(200/360) = 80/200 = 0.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

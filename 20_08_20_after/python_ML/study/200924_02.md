python 

```
Data quality problems
데이터의 최대/최소가 다름 à Scale에 따른 y값에 영향
Ordinary 또는 Nominal 한 값 들의 표현은 어떻게?
잘못 기입된 값들에 대한 처리
값이 없을 경우는 어떻게?
극단적으로 큰 값 또는 작은 값들은 그대로 놔둬야 하는가?

Data preprocessing issues
데이터가 빠진 경우 (결측치의 처리)
라벨링된 데이터(category) 데이터의 처리
데이터의 scale의 차이가 매우 크게 날 경우

데이터가 없을 때 할 수 있는 전략
데이터가 없으면 sample을 drop
데이터가 없는 최소 개수를 정해서 sample을 drop
데이터가 거의 없는 feature는 feature 자체를 drop
최빈값, 평균값으로 비어있는 데이터를 채우기

drop을 통해 값을 줄임

줄일 떄 Nan 데이터를 사라지게 하고 싶다면 df.dropna()를 사용하면 nan이 다 사라짐!

df.dropna(how="all")
np.nan (nan을 생성 - column) / df.dropna(thresh =3 ) => 데이터가 최소 4개이상 없을 때 drop / 

df.mean()평균 / df.median() 중간값 / df.mode() 최빈 값

df.fillna(0) =>데이터가 없는 곳은 0으로 집어 넣기!

notnull() 낫널을 표시!

이산형 데이터 처리

ont hot encoding(데이터 집합 / 실제 데이터 set의 크기만큼 binary Feature를 생성)
ex) {green} : [1,0,0] / {blue}: [0,1,0]

ordiary data -> one hot encoding!

Data Binning(데이터 구간 나누기)
구간명을 설정하고 bins를 두고 categories에 할당!!

data binning (기존 dataframe에 할당)

Label encoding by sklearn
Scikit-learn의 preprocessing 패키지도 label, one-hot 지원
Encoder 생성 / Data에 맞게 encding fitting / 실제데이터 -> labelling data

Label encoder의 fit과 transform의 과정이 나눠진 이유는
- 새로운 데이터 입력시, 기존 labelling 규칙을 그대로 적용할
필요가 있음
Fit 은 규칙을 생성하는 과정
Transform은 규칙을 적용하는 과정
Fit을 통해 규칙이 생성된 labelencoder는 따로 저장하여
새로운 데이터를 입력할 경우 사용할 수 있음
Encoder들을 실제 시스템에 사용할 경우 pickle화 필요

One-hot encoding by sklearn
Numeric labelling이 완료된 데이터에 one-hot 적용
데이터는 1-dim 으로 변환하여 넣어 줄 것을 권장

Feature scaling
Feature간의 최대-최소값의 차이를 맞춘다!

Feature scaling 전략
in-Max Normalization
x(i)_norm = (x(i)  -xmin) / (xmax - xmin) * (new max -  new low) + new low
기존 변수에 범위를 새로운 최대-최소로 변경 /일반적으로 0과 1 사이 값으로 변경함
최소 12,000 / 최대 98,000 -> 기존 값 73,600

Standardization (Z-score Normalization)
x(i)_std_norm = (x(i) -µ) /si
기존 변수에 범위를 정규 분포로 변환
실제 Mix-Max의 값을 모를 때 활용가능

실제 사용할 때는 반드시
정규화 Parameter(최대/최소, 평균/표준편차) 등을
기억하여 새로운 값에 적용해야함

Feature scaling with sklearn
Label encoder와 마찬가지로, sklearn도 feature scale 지원
MinMaxScaler와 StandardScaler 사용

Preprocessing은 모두 fit à transform의 과정을 거침
이유는 label encoder와 동일
단, scaler는 한번에 여러 column을 처리 가능
```
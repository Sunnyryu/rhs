# 머신러닝

```
특성 교차는 두 개 이상의 특성을 곱하여(교차하여 특성 공간에서 비선형성을 인코딩하는) 구성되는 합성 특성

(여러 특성을 교차하면 각 특성의 개별적인 예측 기능 이상의 기능을 이용할 수 있음!)

[A X B]: 두 특성의 값을 곱하여 구성되는 특성 교차
[A x B x C x D x E]: 다섯 개의 특성 값을 곱하여 구성되는 특성 교차
[A x A]: 단일 특성을 제곱하여 구성되는 특성 교차

A와 B가 빈과 같은 부울 특성인 경우 곱의 결과 범위가 매우 희소하게 나타날 수 있음!

특성 교차사용해야하는 이유
(선형 학습자는 대량의 데이터에 맞게 적절히 확장 / 하지만 특성 교차가 없음녀 모델을 충분히 표현할 수 없음 / 특성 교차와 대량의 데이터를 사용하면 매우 복잡한 모델을 효율적으로 학습가능!)
```

```
버킷화(비닝) 특성

ex) 인구로 버킷을 나눴다고 가정(3개로!)
bucket_0 (< 5000): 인구가 적은 지역에 해당
bucket_1 (5000 - 25000): 인구가 중간 정도인 지역에 해당
bucket_2 (> 25000): 인구가 많은 지역에 해당
[[10001], [42004], [2500], [18000]] => [[1], [2], [0], [1]]

특성 값은 이제 버킷 색인입니다. 이러한 색인은 불연속 특성으로 간주됩니다. 이러한 특성은 위와 같이 원-핫 표현으로 변환되는 것이 일반적이지만 이 과정은 투명하게 이루어짐

버킷화 특성에 대한 특성 열을 정의하려면 numeric_column을 사용하는 대신 bucketized_column을 사용합니다. 이 특성 열은 입력으로 취한 숫자 열을 boundaries 인수에 지정된 버킷 경계를 사용하여 버킷화 특성으로 변환합니다. 다음 코드에서는 households 및 longitude에 대한 버킷화 특성 열을 정의합니다. get_quantile_based_boundaries 함수는 분위를 기준으로 경계를 계산하므로 각 버킷은 동일한 수의 요소를 포함
```

```
정규화 

예를들어 시험세트가 반복을 하다가 다시 올라가는 상황이 발생! (과적합이 발생!)
우리의 학습 최적화 알고리즘은 모델이 데이터에 얼마나 적합한지 측정하는 손실 항과 모델 복잡도를 측정하는 정규화 항의 함수가 됨

모델의 모든 특성의 가중치에 대한 함수로서의 모델 복잡도
0이 아닌 가중치를 사용하는 특성의 총 개수에 대한 함수로서의 모델 복잡도

모델 복잡도에 페널티 부여
(가능하면 모델 복잡도를 방지하려고 함!, 학습 단계에서 수행하는 최적화에 이 아이디어를 적용할 수 있음, 구조적 위험 최소화(학습 오류를 낮춰주는 것이 목표!), 또한 복잡도를 낮출 수 있음!)

정규화 - 복잡도(모델)를 정의하는 방법 , 더 작은 가중치 선호, 여기에서 벗어나면 비용 발생

L2(정규화 , 일명 능선) - 복잡도(모델) = 가중치의 제곱의 합 , 아주 큰 가중치에 대한 페널티 부여

모델 개발자는 람다라는 스칼라(정규화율이라고도 함)를 정규화 항의 값에 곱하여 정규화 항의 전반적인 영향을 조정

최소화 (손실( 데이터|모델) + 람다복잡도(모델))
L2 정규화를 수행하면 모델에 다음과 같은 효과를 줄 수 있습니다.

가중치 값을 0으로 유도(정확히 0은 아님)
정규 (종 모양 또는 가우시안) 분포를 사용하여 가중치 평균을 0으로 유도

람다 값을 높이면 정규화 효과가 강화
다 값을 낮추면 그림 3과 같이 더 평평한 히스토그램이 산출되는 경향이 있음

람다 값이 너무 높으면 모델은 단순해지지만 데이터가 과소적합해질 위험이 있습니다. 그렇게 되면 모델은 유용한 예측을 수행할 만큼 학습 데이터에 대해 충분히 학습하지 못할 수 있음

람다 값이 너무 낮으면 모델은 더 복잡해지고 데이터가 과적합해질 위험이 있습니다. 모델이 학습 데이터의 특수성을 너무 많이 학습하게 되고 새로운 데이터로 일반화하지 못하게 되여
람다를 0으로 설정하면 정규화가 완전히 제거됩니다. 이 경우 학습이 손실을 최소화하는 데에만 초점을 맞추게 되어 가장 높은 수준의 과적합 위험을 유발

조기 중단이란 모델이 완전히 수렴되기 전에 학습을 끝내는 것을 뜻
```

```
EX1)    
100개의 입력 특성이 있는 선형 모델 / 10개는 매우 유용, 90는 유용X
모든 특성이 -1과 1 사이의 값을 갖는다고 가정했을 때 참인것

1)L2 정규화로 인해 모델이 일부 유용하지 않은 특성에 대해 적정 가중치를 학습하게 될 수 있음(유용한 특성에 부여되야할 크레딧의 일부를 그러한 유용하지 않은 특성에 잘못 부여하게 되연!)

2)L2 정규화는 많은 유용하지 않은 가중치를 0.0에 가깝게 유도함
```

```
ex2) 두 특성은 서로 거의 동일하지만 둘 중 하나의 특성에는 임의의 노이즈가 조금 포함되어 있습니다. L2 정규화를 사용해 이 모델을 학습한다면 이러한 두 특성에 대한 가중치는 어떻게 될까?
(l2 정규화는 큰 가중치에 더 패널티를 줘서  줄어들도록 강제하기에 비슷해짐!)
두 특성 모두 대체로 동일하며 적정한 가중치를 갖게 됩니다.
L2 정규화는 두 가지 특성 중 하나만 모델에 포함한 경우의 대략 절반 수준인 대체로 동일한 가중치를 향하여 특성을 이끌어 갑니다.
```

```
로지스틱 회귀!
정확히 0 또는 1을 예측하는 대신 확률(0과 1 사이의 값, 0과 1은 제외)을 생성
(많은 문제에 확률 추정치가 출력으로 필요, 로지스틱 회귀를 입력함 , 확률 추정치가 보정되므로 편리, 이진 분류가 필요한 경우에도 유용!)

로지스틱 회귀 모델의 출력이 어떻게 항상 0과 1 사이에 포함되는지 궁금할 수도 있습니다. 공교롭게도 다음과 같이 정의된 시그모이드 함수가 동일한 특성을 갖는 출력을 생성(시그모이드 함수: z가 로지스틱 회귀를 사용하여 학습된 모델의 선형 레이어의 출력을 나타내는 경우 sigmoid(z)는 0과 1 사이의 값(확률)을 생성)
```

```
z가 로지스틱 회귀를 사용하여 학습된 모델의 선형 레이어의 출력을 나타내는 경우 sigmoid(z)는 0과 1 사이의 값(확률)을 생성합니다. 수학적 표현으로는 다음과 같습니다.

여기서

y'는 특정 예에 관한 로지스틱 회귀 모델의 출력입니다.
z = b + w1x1 + w2x2 + ... wNxN
w 값은 모델의 학습된 가중치이고, b는 편향입니다.
x 값은 특정 예에 대한 특성 값입니다.
z는 z를 '1' 라벨(예: '개가 짖음')의 확률을 '0' 라벨(예: '개가 짖지 않음')의 확률로 나눈 값의 로그로 정의할 수 있는 시그모이드 상태의 역수이므로 로그 오즈(log-odds)라고도 합니다.

다음은 ML 라벨이 포함된 시그모이드 함수입니다.
```


```
정규화가 중요함(점금선을 기억, 고차원에서 계속 손실을 0으로 만들려고 시도)

l2정규화(l2 가중치감소): 아주 큰 가중치에 페널티 줌 / 조기중단: 학습단계 또는 학습률 제한

선형 로지스틱 회귀는 매우 효율적 / 학습 및 예측시간이 매우빠름 , 짧고 뚱뚱한 모델이 RAM을 많이 사용

L1 정규화(L1 regularization)
가중치의 절대값 합에 비례하여 가중치에 페널티를 주는 정규화 유형 (희소 특성에 의존하는 모델에서 L1 정규화는 관련성이 없거나 매우 낮은 특성의 가중치를 정확히 0으로 유도하여 모델에서 해당 특성을 배제하는 데 도움이 됌)
```

```
그 오즈(log-odds)
어떤 이벤트가 일어날 가능성의 로그입니다.

이벤트가 이진 확률을 의미하는 경우의 가능성은 성공 확률(p) 대 실패 확률(1-p)의 비율을 의미합니다. 특정 이벤트의 성공 확률이 90%, 실패 확률이 10%라고 가정해 보겠습니다. 이 경우의 가능성은 다음과 같이 계산됩니다.

로그 오즈는 가능성의 로그입니다. 관례에 따르면 '로그'란 자연 로그를 나타내지만 로그의 밑은 사실 1보다 큰 임의의 수가 될 수 있습니다. 관례를 따르면, 앞에서 든 예의 로그 오즈는 다음과 같이 나타낼 수 있습니다.

로그 오즈는 시그모이드 함수의 역함수입니다.
```
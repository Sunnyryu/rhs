# 머신러닝

```
역전파: 숙지할 사항
경사의 중요성(역전파에서는 학습할 사례를 서로 변별 가능하게 해주는 '경사'라는 개념이 매우 중요)
미분 가능하면 학습이 가능할 확률이 높음

입력 쪽에 가까운 하위 레이어의 경사가 매우 작아질 수 있습니다. 심층 네트워크에서 이러한 경사를 계산할 때는 많은 작은 항의 곱을 구하는 과정이 포함
하위 레이어의 경사가 0에 가깝게 소실되면 이러한 레이어에서 학습 속도가 크게 저하되거나 학습이 중지 -> ReLU 활성화 함수를 통해 경사 소실을 방지

네트워크에서 가중치가 매우 크면 하위 레이어의 경사에 많은 큰 항의 곱이 포함 -> 경사가 너무 커져서 수렴하지 못하고 발산하는 현상이 나타날 수 있음 (batch 정규화를 사용하거나 학습률을 낮추면 경사 발산을 방지)

ReLU 유닛의 가중 합이 0 미만으로 떨어지면 ReLU 유닛이 고착될 수 있습니다. 이러한 경우 활동이 출력되지 않으므로 네트워크의 출력에 어떠한 영향도 없으며 역전파 과정에서 경사가 더 이상 통과할 수 없음
 (학습률을 낮추면 ReLU 유닛 소멸 방지!)


batch 정규화(유용한 노브)로 해결 가능

선형 조정
입력값을 -1, 1 범위에 들어오도록 정규화하는 것이 권장되는 표준 방식입니다. 이렇게 하면 SGD에서 한 차원으로 너무 크거나 다른 차원으로 너무 작은 단계를 밟을 때 고착을 방지하는 데 도움이 됨!

드롭아웃(단일 경사 스텝에서 유닛 활동을 무작위로 배제하는 방식): 또 하나의 정규화 형태, NN에 유용

단일 경사 스텝에서 네트워크의 유닛을 무작위로 배제
앙상블 모델과의 접점
드롭아웃이 많을수록 정규화가 강력해짐
0.0 = 드롭아웃 정규화 없음
1.0 = 전체 드롭아웃. 학습 중지
중간 범위의 값이 유용함
```

```
일대다(이진 분류 활용 방법을 제공)

클래스가 세 개 이상인 경우!
로지스틱 회귀는 이진 클래스 문제에 유용한 확률을 제공합니다.
스팸/스팸 아님, 클릭/클릭 안함
다중 클래스 문제는 어떤가요?
사과, 바나나, 자동차, 심장병 전문의, ..., 보행 신호, 얼룩말, 동물원
빨간색, 주황색, 노란색, 녹색, 파란색, 남색, 보라색
동물, 식물, 광물

일대다 다중 클래스
각각의 가능한 클래스의 고유한 출력을 만듬!
'내 클래스' 대 '다른 모든 클래스'의 신호에 대해 학습가능!
심층망에서나 별도의 모델을 사용하여 실행할 수 있음!
```

```
소프트맥스(소프트맥스는 출력 레이어 바로 앞의 신경망 레이어를 통해 구현)


소프트맥스 다중 클래스
제약조건 추가: 모든 일대다 노드의 합이 1.0이 되어야 합니다.
추가 제약조건은 빠르게 수렴을 학습하는 데 도움이 됩니다.
또한 출력을 확률로 해석할 수 있습니다.

소프트 맥스 옵션
전체 소프트맥스는 즉 모든 가능한 클래스의 확률을 계산하는 소프트맥스(무차별 대입)

후보 샘플링
모든 양성 라벨에 대해 계산하지만 음성 라벨의 무작위 샘플에 대해서만 계산합니다

전체 소프트맥스는 클래스 수가 적으면 매우 적은 비용이 들지만 클래스 수가 증가하면 엄청나게 많은 비용이 듭니다. 클래스 수가 많은 문제에서 후보 샘플링을 사용하면 효율성이 향상

다중 클래스, 단일 라벨 분류:
하나의 예는 한 클래스만의 멤버가 될 수 있습니다.
클래스가 상호 배타적인 제약조건이 유용한 구조입니다.
손실에서 인코딩하는 데 유용합니다.
모든 가능한 클래스에 하나의 소프트맥스 손실을 사용합니다.

다중 클래스, 다중 라벨 분류:
하나의 예가 두 개 이상의 클래스의 멤버가 될 수 있습니다.
클래스 멤버에 추가로 적용되는 제약조건이 없습니다.
각각의 가능한 클래스에 하나의 로지스틱 회귀 손실이 있습니다.




```
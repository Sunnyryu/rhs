# 머신러닝(클러스터링 2)

```
감독적 유사성 측정

수동으로 결합된 형상 데이터를 비교하는 대신 형상을 줄일 수 있음

데이터 호출 임베딩 / 임베딩 비교, 임베딩은 감독된 훈련(DNN)에 활성화 되었으며, 피쳐데이터는 그것에 영향을...

기능의 네트워크(DNN) 자료 자체 임베딩은 피쳐 데이터를 임베딩의 벡터에 매핑
공간은 피쳐데이터보다 더 적은 수치를 가능!
피쳐 데이터 세트의 잠재 구조를 캡처하는 방식

유사한 예시를 위한 벡터 내장 동일한 사용자가 임베딩 공간에서 서로 가까워짐
유사성 측도는 이 "밀도"를 사용하여 두 쌍의 유사성을 수량화

수동 또는 감독에 관계없이 유사성 측정이 사용

요구사항 / (매뉴얼 / 감독된!)
상관된 기능에서 중복된 정보를 제거 (피쳐 간의 상관 관계를 별도 조사 / DNN은 중복된 정보 제거)
계산된 유사성에 대한 통찰력을 제공 ( 예 / 아니요 내장재는 디시핑 불가)
기능이 거의 없는 소규모 데이터 셋에 적합( 예, 몇 가지 기능으로 수동으로 설계하는 게 쉬움 / 아니요 소규모 데이터 셋은 DNN에 대한 충분한 훈련 데이터 제공 X)
다양한 기능을 갖춘 대형 데이터 셋에 적합 ( 아니요, 여러 기능에서 중복된 정보를 수동으로 제거, 그것들을 결합하는 것은 매우 어려움 / 예 DNN은 자동으로 중복 정보를 제거하고 겸비!)

감독적 유사성 측정 프로세스

(피쳐데이터 삽입 -> DNN 선택 (오토 인코더, 예측기) -> 임베딩 추출 -> 측정 선택( 생산아님, 유클리언 거리!))

교육 레이블을 기준으로 DNN 선택

동일한 기능을 사용하는 DNN을 교육하여 기능 데이터를 내장형으로 축소 입력과 레이블 모두에서 데이터를 피쳐링함

집의 경우 데이터, DNN은 가격, 크기 및 우편 번호와 같은 기능을 사용할 수 있음 (특징들을 스스로 예측)
형상 데이터를 사용하여 예측하기 위해 동일한 기능 데이터, DNN은 입력 기능 데이터를 다음으로 줄여야 함!

입력 데이터 자체를 예측하여 입력 데이터 임베딩을 학습하는 DNN 는 오토엔코더라고 불림!
자동 인코더의 숨겨진 레이어가 더 작기 때문
입력 및 출력 계층보다 자동 엔코더가 강제로 학습
입력 기능 데이터의 압축 표현 일단 DNN이 훈련을 받으면 마지막 숨겨진 레이어에서 임베딩을 추출하여 유사도를 계산

오토 엔코더 -> 피쳐 삽입, 히든 레이어 / 프레디터 -> 피쳐 삽입, 히든 레이어, 피쳐 키

자동 인코더는 임베디딩을 생성하는 가장 간단한 방법
a 특정 기능이 더 많을 수 있는 경우 자동 인코더는 최적의 선택이 아님
모든 입력 기능을 예측하는 대신 특정 입력 기능을 예측합니다. 예측 변수 DNN (형상을 선택)

손실 때문에 레이블로 범주형 형상에 숫자 형상을 선호한다. 숫자 형상에 대해 계산하고 해석하는 것이 더 쉬움
카디널리티 ≲와 함께 범주형 피쳐 사용 안 함 라벨로는 100입니다. 이 경우 DNN은 입력 데이터를 임베딩으로 줄이도록 강제하지 않음 (왜냐하면 DNN은 쉽게 낮은 카디널리티 범주형 레이블을 예측할 수 있기 때문)

DNN (기능 줄이기) 
DNN의 모든 출력에 대한 손실을 계산 
변수 /  mean square error (MSE) 
모든 출력에 대한 손실을 합산하여 총 손실을 계산 
```

```
임베딩 생성 예제 (감독된 사람에게 사용된 내장재를 생성하는 방법을 보여줌!)
데이터를 생성할 때 사용한 것과 동일한 하우징 데이터 세트를 가지고 있다고 가정

가격 / 양의 정수 , 크기 / 제곱미터 단위의 양의 부동 소수점 값 , 우편번호 / 정수 , 침실 수 / 정수 , 주택의 종류 / 다세대, 아파트, 콘도 , 차고지 / (0/1 아니요/예), 색 / 노란색,녹색

데이터 사전 처리 (피쳐 데이터를 입력으로 사용하기 전에 데이터를 미리 처리해야 함! )

가격 / 포아송 분포 / 수량화 및 [0,1]로 확장함
크기 / 포아송 분포 / 수량화 및 [0,1]로 확장함
우편번호 / 범주형 / 경도와 위도로 변환하고 정량화하여 [0,1]로 스케일링함
침실수 / 정수 / 특이치를 장착하고 [0,1]로 확장하십시오.
주택의 종류 / 범주형 / 단일 핫 인코딩으로 변환 
차고지 / 0 OR 1 / 그대로 놔둠 
색 / 범주형 / RGB 값으로 변환 숫자데이터 처리

예측 변수 또는 자동 인코더 선택

임베디딩을 생성하려면 자동 인코더 또는 자동 인코더를 선택할 수 있음 / 예측 변수 기본 선택은 자동 인코더
예측 변수 대신 데이터 집합의 특정 기능이 결정하는 경우 유사성 완전성을 위해 두 가지 경우를 모두 살펴봄

예측 변수 양성
DNN에 대한 교육 레이블로 선택해야함 / 예시 간의 유사성 결정하는 데 중요한 역할 
훈련용 라벨로 가격을 선택하고 입력 기능 데이터에서 다음을 수행
다른 모든 기능을 입력 데이터로 사용하여 DNN을 훈련 / 
손실함수는 단순히 예측가격과 실제 가격 사이의 MSE / 

오토엔코더 양성
자동 인코더를 훈련 / 자동 인코더의 숨겨진 레이어가 입력보다 작아야함
감독된 유사성에 설명된 대로 각 출력에 대한 손실 계산
각 출력에 대한 손실을 합하여 손실 함수를 만듬 (모든 특징에 대해 손실의 가중치를 균등하게 매기다
 예를 들어 색상 데이터가 다음과 같기 때문에 RGB로 처리되어 각 RGB 출력에 1/3초씩 가중치를 부여)

DNN에서 임베딩 추출
DNN을 교육한 후 예측 변수든 자동 인코더든 간에 DNN에서 예제를 위해 임베딩. 를 사용하여 임베딩을 추출 (벡터들은 다른 집들의 경우 벡터보다 비슷한 집들이 더 가까이 있어야 한다.)
```

```
DNN은 중복된 정보를 제거 / 이러한 특성을 입증하려면 적절한 데이터에 대해 DNN을 교육한 다음 수동 유사성 측정의 결과와 비교

계산된 유사점에 대한 통찰력을 제공하지 않음 / 임베딩이 무엇을 의미하는 지 모르기에 통창력이 없음 / 기능이 복잡한 대규모 데이터셋에 적합!
입력 데이터를 이해할 필요가 없다는 것
소규모 데이터셋에는 적합하지 않음. 작은 데이터 집합은 DNN을 훈련하기에 충분한 정보를 가지고 있지
않음
```

```
A점 B점, C점이 있을때 B가 C와 가깝고 비슷하지만 B가 A와 비슷해지기 원한다면 

코사인은 벡터 사이의 각도엠나 의존(크키가 작다) / 도트 제품 (코사인 성분과 거리에 모두 비례 / 벡터 길이, 코사인이 B와C보다 높더라도 길이가 높을수록 유사해짐) / 유클리드 거리(거리가 작을 수록 유사함!)


```
```
당신은 뮤직비디오의 유사성을 계산하고 있습니다. 길이 뮤직비디오의 벡터를 삽입하는 것은 그들의 인기에 비례 (코사인 -> 도트 제품) ? +> 인기 동영상은 일반적으로 모든 동영상과 더 유사 (도트 제품은 두 벡터의 길이에 따라 영향을 받기 때문에 
두 벡터의 길이가 달라지기 때문에 인기 있는 비디오의 벡터 길이가 크면 모든 비디오와 더 비슷해질 것)
```
| 기준  |감독된   | 매뉴얼   |
|---|---| --- |
| 작성기준  | 다음을 통해 생성된 내장물 간 거리   |  기능 데이터를 수동으로 결합   |
|  사용 시기 |  데이터 세트는 크고 기능을 결합하기 어려움 |  데이터 세트는 작고 기능이 쉽게 결합   |
|  시사 |  결과에 대한 통찰력은 없지만 DNN은 변화에 자동으로 적응할 수 있습니다. |    시사에 대한 통찰력 필요, 유사도 계산 결과, 그러나 형상 데이터가 있는 경우 변경 사항을 적용하면 유사성 측도를 업데이트 |


# 머신러닝(추천시스템)

```
홈페이지를 추천해주거나 관련항목을 추천해주는 좋은 서비스도 포함

아이템 (문서라고도 함!)
시스템이 권장하는 엔티티 (구글 앱의 경우 항목은 앱 / 유튜브는 동영상!)

쿼리(컨텍스트라고 함!) 
시스템이 권장 사항을 만드는 데 사용하는 정보 
(ex
사용자 정보 (ID, 사용자가 이전에 상호작용한 항목)
추가맥락(시시각각, 사용자의 장치!)
)

이산 집합으로부터의 매핑(이 경우, 쿼리 집합 또는 쿼리 집합) 권장할 항목)을 내장 공간이라고 하는 벡터 공간으로 보냄
```

```
권장시스템은 후보 세대 / 점수 / 랭크 재지정으로 구성됨! (아키텍처 구성요소)

후보세대 (메트릭스 인자화/dnn도 후보세대의 종류 중 하나)
(시스템은 잠재적으로 거대한 말 뭉치로부터 시작 -> 적은 수의 후보군 생선 -> 신속하게 쿼리 평가 필요 -> 모델은 여러개의 후보생성자를 제공 가능!)

점수
다른 모델이 점수를 매기고 후보 순위를 매겨 선정 -> (사용자에게 표시할 세트 보여줌) -> 모델은 시스템이 사용할 수 있는 작은 부분 집합 평가 -> 추가 쿼리에 의존하는 보다 정밀한 모델도...

랭크 재지정
시스템은 시스템에 대한 추가 제약 조건을 고려 -> (시스템은 사용자가 항목을 제거) -> 더 신선한 내용의 점수를 명시적으로 싫어하거나 높임 -> 재지정 또한 다양성 신선함 공정성을 보장하는 데 도움을 줌!
```

```
후보 세대 개요

후보 세대는 1단계 추천 / 쿠러 지정 -> 시스템은 일련의 관련 후보 생성 

내용 기반 필터링 (항목 간의 유사성을 사용하여 항목 권장 사용자가 좋아하는 것과 유사)
협업 필터링 (쿼리와 항목 간의 유사성을 동시에 사용합니다 권장 사항을 제공하려고 함!)

임베딩 스페이스 

컨텐츠 기반 및 협업 필터링 모두 각 항목과 각 쿼리(또는 컨텍스트)를 공통 내장 공간의 내장 벡터에 매핑
전형적으로 내장공간은 저차원
ex) 사용자가 보는 유튜브 영상과 같은 유사한 아이템은 임베딩 공간에 가깝게 붙음(밀착성의 개념은 유사성 측도로 정의!)

유사성 

유사도 측정은 임베딩 한 쌍을 취하여 유사도를 측정하는 스칼라를 반환하는 함수
(즉 임베딩에 가까운 아이템 임베딩들을 찾음)

코사인 , 닷 프로덕트, 유클리드 거리라는 요소가 유사도를 결정하는 요소!

코사인(단순히 두 벡터 사이의 각도) 
닷 프로덕트 (코사인의 각도 x 프로덕트의 산물/표준, 임베딩이 정상화되면 코사인과 일치)
유클리드 거리(거리가 작으면 유사성이 높음! / 임베딩이 정상화하면 유클리드 거리는 닷 프로덕트와 상수까지 일치!)
```

```
콘텐츠 기반 필터링은 아이템 기능을 사용하여 사용자가 좋아하는 것과 유사한 다른 아이템을 이전 행동이나 명시적 피드백을 바탕으로 추천

모델은 이 사용자와 관련된 항목을 추천해야 한다. 그렇게 하려면 먼저 유사성 메트릭(예: 도트 제품)을 선택해야 한다
이 유사성 메트릭스에 따라 각 후보 항목의 점수를 매기도록 시스템을 설정해야 한다
다. 모델이 다른 사용자에 대한 정보를 사용하지 않았으므로 권장사항은 이 사용자에게만 한정된다는 점에 유의

컨텐츠 기반 필터링이 장/단점

장점 
권장사항은 이 사용자에게만 적용되므로 모델에 다른 사용자에 대한 데이터가 필요하지 않음
많은 사용자로 쉽게 확장 가능
모델은 사용자의 특정 관심사를 포착할 수 있으며, 다른 사용자들이 관심을 갖는 틈새 아이템을 추천할 수 있음

단점
항목의 특징 표현은 어느 정도 수작업으로 설계되기 때문에, 이 기법은 많은 도메인 지식이 필요
(물론 모델은 손으로 조작한 특징만큼만 좋음!)
모델은 사용자의 기존 관심사에 근거하여만 권고할 수 있다. 즉, 모델은 이용자의 기존 관심사를 확장하는 데 한계가 있음
```

```
협업 필터링

컨텐츠 기반 필터링의 일부 한계를 해결하기 위해, 협업 필터링은 사용자와 항목의 유사성을 동시에 사용하여 권장사항을 제공 (우연한 추천을 허용 / 협업 필터링 모델은 유사한 사용자 b의 이해관계에 기초하여 사용자 a에게 추천 가능! / 임베딩은 자동으로 학습될 수 있음!!)

1d 임베딩 (1차원적 임베딩 - 한가지 특징을 사용)

2d 임베딩 ( 하나의 특징은 모든 사용자의 선호도를 설명하기에 충분하지 않음 -> 두번 째 특징을 추가함)

임베딩 공간은 유사성 메트릭스를 사용하여 유사성이나 관련성을 측정할 수 있는 항목과 사용자 모두에게 공통되는 추상적 표현이라고 생각할 수 있음
```

```
매트릭스 인자화(단순한 내장 모델임!)

매트릭스 인자화는 일반적으로 전체 매트릭스를 학습하는 것보다 더 압축적인 표현을 제공

전체 행렬에는 항목이 있는 반면, 내장 행렬에는 항목이 있는 반면 내장 차원은 일반적으로 및 및보다 훨씬 작음
행렬 인자화는 관측치가 저차원 하위 공간에 가깝다고 가정하여 데이터에서 잠재된 구조를 찾음
n, m, d의 값은 너무 낮아서 이점은 무시할 수 있음
실제 권고 시스템에서 매트릭스 인자화는 전체 매트릭스를 학습하는 것보다 훨씬 더 압축적일 수 있음

단수 값 분해(SVD)로 이차적 문제를 풀 수도 있음

가중 매트릭스 인자화는 관측된 합목에 대한 합계, 수신되지 않은 항목(제로 처리된)의 합계로 분해

목표 기능 최소화
확률적 경사 강하(SGD)는 손실 기능을 최소화하기 위한 일반적인 방법
가중 교대 최소 제곱(WALS)은 이 특정 목적에 특화되어 있음.

SGD 
매우 유연(다른 손실 기능을 사용 가능) / 평행할 수 있음 
느림(빠르게 수렴되지 않음) / 관찰되지 않은 입력을 처리하기 힘듬

WALS
평행할 수 있음 / SGD보다 수렴이 빠름 / 수신되지 않은 항목을 더 쉽게 처리
오직 정사각형 손실에만 의존
```

```
협업필터링의 장/단점

장점
도메인 지식이 필요하지 않음 (임베딩이 자동으로 학습되므로)

의도하지 않은 발견(Serendipity)
사용자들이 새로운 관심사를 발견하도록 도울 수 있음

좋은 출발점
시스템은 매트릭스 인자화 모델을 훈련시키기 위해 피드백 매트릭스만 필요로 하며, 상황별 특징을 필요로 하지 않음

단점
새항목을 처리할 수 없음
(주어진 (사용자, 항목) 쌍에 대한 모델의 예측은 해당 임베딩의 도트 제품 / 훈련중에 항목이 보이지 않으면 시스템에 임베딩 할 수 없고, 모델을 조회할 수 없음 -> 콜드 스타드 문제)

WALS를 이용하면 위의 단점을 어느정도는 해결할 수 있긴함 

새로운 항목이 임베딩을 생성을 위한 휴리스틱스를 이용하면 어느정도 임베딩의 근사치를 산출하여 해결할 수 있음

쿼리/항목의 측면 피쳐를 포함하기 어려움
(측면 피쳐는 쿼리 또는 항목 ID를 벗어난 모든 피쳐)

위의 경우도 WALS에 사이드 피쳐를 포함하거나 일반화한다면 해결 할 수 있음


```
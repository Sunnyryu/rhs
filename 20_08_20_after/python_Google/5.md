# 머신러닝

```
분류

확률 결과에 로지스틱 회귀(로지스틱 회귀는 확률을 반환)를 사용하기도 하는데, 이 회귀의 형태는 (0, 1)입니다.
다른 경우 별개의 이진 분류 값에 임계값(분류 임계값(결정 임계값이라고도 함))을 설정합니다.
임계값 선택은 중요하며 값을 조정할 수 있습니다.

정확성(올바른 예측의 비율, 정확한 예측수 / 총 예측수 => (TP+TN /(TP+TN+FP+FN)))
- 정확성의 오류
다양한 종류의 실수에 여러 비용이 소요되는 경우가 많습니다.
대표적인 경우로 양성이나 음성이 거의 없는 클래스 불균형을 들 수 있습니다.
(TP와 FP로 해결할 수 있음)

정밀도: (참 양성(TP)) / (모든 양성 예측) => (TP/(TP+FP), 거짓 양성이 나오지 않은 모델의 정밀도는 1.0)
모델이 '양성' 클래스로 나타났을 때 결과가 옳았나요?
직관: 모델이 '늑대다'라고 너무 자주 외쳤나요

재현율: (참 양성(TP)) / (실제 양성 모두) => (TP + (TP+FN), 거짓 음성이 없으면 재현율 1.0)

분류 임계값을 올리면 허위 양성수는 감소/ 허위 음성수 증가 => 정밀도 증가, 재현율 감소
분류 임계값을 내리면 허위 양성수는 증가/ 허위 음성수 감소 => 정밀도 감소, 재현율 증가
모든 양성 가능성 중에서 모델이 몇 개나 정확히 식별했나요?
직관: 모델에서 놓친 늑대가 있나요?
EX) 스팸과 스팸이 아님이 있을 때 분류 임계값을 올리면 어찌될까=> 거짓양성(FP)가 감소하므로 정밀도가 높아짐!

ROC 곡선 -> 모든 분류 임계값에서 분류 모델의 성능을 보여주는 그래프

각 점은 하나의 결정 임계값에 있는 참양성(TP)과 거짓양성(FP) 비율(R)입니다.

TPR = (TP / (TP+FN)), FPR = (FP/ (FP+TN))

AUC: 'ROC 곡선 아래 영역'
AUC 값의 범위는 0~1입니다. 예측이 100% 잘못된 모델의 AUC는 0.0이고 예측이 100% 정확한 모델의 AUC는 1.0입니다

AUC는 척도 불변입니다. AUC는 절대값이 아니라 예측이 얼마나 잘 평가되는지 측정합니다.
AUC는 분류 임계값 불변입니다. AUC는 어떤 분류 임계값이 선택되었는지와 상관없이 모델의 예측 품질을 측정합니다.

해석:
임의의 양성 예측과 임의의 음성 예측을 선택할 때 내 모델이 정확한 순서로 순위를 매길 확률이 얼마나 될까요?
직관: 가능한 모든 분류 임계값에서 집계된 성능의 집계 측정값을 제공합니다.

척도 불변이 항상 이상적인 것은 아닙니다.
분류 임계값 불변이 항상 이상적인 것은 아닙니다.

주어진 모델에서 모든 예측에 2.0을 곱한다고 해도 AUC로 측정한 모델의 예측 성능은 상대적이므로 바뀌지 않음! 


예측 편향(두 평균이 서로 얼마나 멀리 떨어져 있는지 측정하는 수량)
로지스틱 회귀 예측은 편향되지 않아야 합니다.
예측 평균 == 관찰 평균
편향을 보면 시스템의 상태를 판단할 수 있습니다.
편향이 0이라고 해서 시스템 전체가 완벽하다는 것을 의미하지 않습니다.
하지만 상태를 확인하는 데 매우 유용

원인)
불완전한 특성 세트
노이즈 데이터 세트
결함이 있는 파이프라인
편향된 학습 샘플
지나치게 강한 정규화

예측 편향을 줄이기 위해 모델의 결과를 조정하는 캘리브레이션 레이어(calibration layer)를 추가하여 학습한 모델을 사후 처리하는 방법으로 예측 편향을 수정
```

```
EX)양치기 소년을 N번 반복한다고 했을 때
늑대다 (양성클래스)/ 늑대가없다(네거티브 클래스) -2X2 혼동행렬 사용가능
참양성(TP)은 모델에서 포지티브 클래스를 정확하게 평가하는 결과입니다. 마찬가지로 참음성(TN)은 모델에서 네거티브 클래스를 정확하게 평가하는 결과입니다.
거짓양성(FP)은 모델에서 포지티브 클래스를 잘못 예측한 결과입니다. 
거짓음성(FN)은 모델에서 네거티브 클래스를 잘못 예측한 결과입니다.
```
![111](https://i.imgur.com/KcaHPSE.png)
![222](https://i.imgur.com/eZQuquN.png)


```
특성교차

희소 특성 교차는 특성 공간을 크게 늘릴 수 있지만 모델 크기(RAM)이 매우 커질 수 있음 / 노이즈 계수(과적합의 원인)

L1 정규화
L0 가중치 기준에 페널티를 주고자 함 (볼록하지 않은 최적화, NP-난해)
L1 정규화로의 완화 ( 절대값(가중치)의 합에 페널티 줌, 볼록문제 , L2와 달리 희소성을 유도)

L2와 다르게 L1은 유용한 특성히 정확히 0.0의 가중치를 얻게 하려고 합니다.
(약하게나마 유용한트성, 다양한 조정 상황에서 매우 유용한 특성, 비슷하게 유용한 다른 특성과 상관관계가 높은 유용한 특성은 0이 나올 수 있음 ) / 대부분의 유용하지 않은 가중치도 정확히 0.0으로 되려고 함!

L2는 특성 개수를 줄이는 일이 거의없지만 L1은 특성 개수를 줄이는 경향이 있기에 더 작은 모델을 만들 수 있음!!
```

```
신경망(보다 정교한 버전의 특성교차!)

출력(히든레이어들의 가중합!) - 히든레이어들(입력 노드 값의 가중합!) - 입력

활성화 함수(비선형 변환 레이어)
비선형 문제를 모델링하기 위해 비선형성을 직접 도입할 수 있음!

일반적인 활성화 함수
다음 시그모이드 활성화 함수는 가중 합을 0과 1 사이의 값으로 변환합니다.

정류 선형 유닛(ReLU) 활성화 함수는 시그모이드와 같은 매끄러운 함수보다 조금 더 효과적이지만, 훨씬 쉽게 계산

ReLU의 우월성은 경험적 결과를 바탕으로 하며, 이는 ReLU의 반응성 범위가 더 유용하기 때문임 (사실 어떠한 수학 함수라도 활성화 함수의 역할을 할 수 있습니다. 가 활성화 함수(ReLU, 시그모이드 등)를 나타낸다고 가정)

뉴런과 유사한 노드 집합이 레이어에 구성되어 있습니다.
각 신경망 레이어와 하위 레이어와의 연결을 나타내는 가중치 집합이 있습니다. 하위 레이어는 또 다른 신경망 레이어이거나 유형이 다른 레이어일 수도 있습니다.
편향 집합이 각 노드에 하나씩 존재합니다.
레이어에서 각 노드의 출력을 변환하는 활성화 함수가 있습니다. 레이어에 따라 활성화 함수가 다를 수 있습니다.


```

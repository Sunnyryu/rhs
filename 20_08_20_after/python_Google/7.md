# 머신러닝

```
임베딩은 고차원 벡터의 변환을 통해 생성할 수 있는 상대적인 저차원 공간을 가리킵니다

협업 필터링은 다른 여러 사용자의 관심분야를 바탕으로 특정 사용자의 관심분야를 예측하는 작업

d차원 임베딩
영화에 대한 사용자의 관심분야를 대략 d개의 측면에서 설명할 수 있다고 가정합니다.
각각의 영화는 차원 d의 값이 해당 측면에 대한 각 영화의 일치도를 나타내는 d차원 지점이 됩니다.
임베딩은 데이터를 통해 학습할 수 있습니다.

(중요한 것은 주어진 차원에서 특정 영화의 값이라기보다는 임베딩 공간에서 영화들 간의 거리값)

범주형 데이터란 선택사항이 유한한 집합에 속한 하나 이상의 이산 항목을 표현하는 입력 특성을 가리킴
범주형 데이터(categorical data)는 대부분의 요소가 0인 희소 텐서를 통해 가장 효율적으로 표현
(예를 들어 영화 추천 모델을 개발한다고 하면 그림 3에서 확인할 수 있는 것처럼 모든 영화에 고유 ID를 부여한 후 사용자가 본 영화의 희소 텐서로 사용자를 나타낼 수 있음)

터에 더 큰 텍스트 뭉치에 있는 단어의 수가 포함될 수 있습니다. 이는 'BOW(bag of words)' 표현으로 알려져 있습니다. BOW 벡터에서는 500,000개의 노드 중 여러 개의 노드의 값이 0이 아닐 수 있음


망 크기
데이터의 양. 모델의 가중치가 많을수록 효과적인 학습을 위해 더 많은 데이터가 필요합니다.

계산량. 가중치가 많을수록 모델을 학습하고 사용하는 데 더 많은 계산이 필요합니다. 따라서 하드웨어가 이를 지원하지 못할 가능성이 높습니다.

망 축소하기

임베딩은 하나의 행렬이고, 행렬의 각 열은 어휘 항목 하나에 대응

하지만 희소한 BOW(bag of words) 벡터는 어떻게 변환해야 할까요? 여러 개의 어휘 항목(예: 문장 또는 단락의 모든 단어)을 나타내는 희소 벡터에 대한 밀집 벡터를 얻으려면 개별 항목에 대해 임베딩을 검색한 다음 이를 전부 더하면 됩니다.

희소 벡터에 어휘 항목의 수가 포함되어 있으면 각 임베딩에 해당 항목의 수를 곱한 다음 이를 합계에 추가할 수 있습니다.

방금 설명한 검색, 곱셈, 덧셈 절차는 행렬 곱셈과 동일합니다. 1xN 크기의 희소 표현 S와 NxM 크기의 임베딩 표 E가 주어지면 행렬 곱셈 SxE를 통해 1xM 밀집 벡터를 얻을 수 있음

주성분 분석(PCA)은 단어 임베딩을 만드는 데 사용되어 왔습니다. BOW 벡터와 같은 인스턴스의 집합이 주어지면 PCA는 단일 차원으로 축소할 수 있는 높은 상관도를 갖는 차원을 찾음

Word2vec은 분포 가설에 기반하여 의미론적으로 유사한 단어를 기하학적으로 가까운 임베딩 벡터로 매핑

심층망(심층신경망 -DNN)에서 임베딩 학습
임베딩 레이어는 차원당 단위 하나를 갖는 히든 레이어에 불과하므로 별도의 학습 과정이 필요하지 않습니다.
지도 정보(예: 사용자가 동일한 영화 2개를 시청함)를 통해 원하는 작업에 맞게 학습된 임베딩을 조정합니다.
히든 단위는 최종 목표를 최적화하도록 d차원 공간에서 항목을 정리하는 방법을 직관적으로 발견합니다.


희소 데이터 또는 임베딩하려는 밀집 데이터가 있는 경우 크기가 d인 특수 유형의 은닉 단위인 임베딩 단위를 만들 수 있습니다. 이 임베딩 레이어는 다른 특성 및 히든 레이어와 결합할 수 있습니다. 모든 DNN(심층신경망)이 그러하듯, 최종 레이어는 최적화 중인 손실

다른 사용자의 관심분야를 바탕으로 특정 사용자의 관심분야를 예측하기 위한 협업 필터링 작업을 한다고 가정합니다. 이 작업에서 사용자가 시청한 소수의 영화를 양의 라벨로 무작위로 따로 분류하여 지도 학습 문제로 모델링한 다음 소프트맥스 손실을 최적

DNN의 일부로서 부동산 광고에 나온 단어에 관해 임베딩 레이어를 만들어 주택 가격을 평가하려 할 때, 학습 데이터에 있는 알려진 주택 판매가를 라벨로 사용하여 L2 손실을 최적화할 수 있습니다.

d차원 임베딩을 학습할 때, 각 항목은 d차원 공간의 지점에 매핑되어 유사한 항목이 이 공간에서 서로 가까이 위치하게 됩니다. 그림 6은 임베딩 레이어에서 학습한 가중치와 기하학적 보기 간의 관계를 보여줍니다. 입력 노드와 d차원 임베딩 레이어에 있는 노드 간의 에지 가중치는 d개 축 각각의 좌표 값과 일치
```

```
예시
감정 분석 모델을 학습시켜 리뷰가 전반적으로 긍정적(라벨 1)인지 아니면 부정적(라벨 0)인지를 예측
문자열 값인 단어를 어휘, 즉 데이터에 나올 것으로 예상되는 각 단어의 목록을 사용하여 특성 벡터로 변환
각 단어는 특성 벡터의 좌표에 매핑
의 문자열 값인 단어를 이 특성 벡터로 변환하기 위해, 예 문자열에 어휘 단어가 나오지 않으면 각 좌표의 값에 0을 입력하고 어휘 단어가 나오면 1을 입력하도록 인코딩하겠습니다. 예의 단어 중 어휘에 나오지 않는 단어는 무시(명시적인 어휘를 만드는 대신 각 단어를 해시하는 특성 해싱 접근법)

입력 파이프라인 구축 -> 희소 입력 및 명시적 어휘와 함께 선형 모델 사용(라이너분류) ->  심층신경망(DNN) 모델 사용(라이너분류 -> DNN분류로 교체)
-> DNN 모델에 임베딩 사용 -> 임베딩 조사 ->  모델 성능 개선 시도(초매개변수 변경 / ADAM등의 다른 옵티마이저 사용, Informative_terms에 더 많은 단어추가!)

임베딩을 사용한 DNN 솔루션이 원래의 선형 모델보다 우수할 수 있지만, 선형 모델도 성능이 그다지 나쁘지 않았으며 학습 속도는 상당히 더 빨랐습니다. 선형 모델의 학습 속도가 더 빠른 이유는 업데이트할 매개변수 또는 역전파할 레이어의 수가 더 적기 때문입니다.

LinearClassifier 또는 DNNClassifier를 학습시킬 때 희소 열을 사용하려면 어댑터가 필요합니다. TF는 embedding_column 또는 indicator_column이라는 두 가지 옵션을 제공
```
